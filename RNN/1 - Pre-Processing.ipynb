{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pagan133\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\computation\\expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./classifica-serie-a/2019-01-01_2019-08-26_0.csv\n",
      "./classifica-serie-a/2019-08-27_2019-09-01_1.csv\n",
      "./classifica-serie-a/2019-09-02_2019-09-16_2.csv\n",
      "./classifica-serie-a/2019-09-17_2019-09-22_3.csv\n",
      "./classifica-serie-a/2019-09-23_2019-09-26_4.csv\n",
      "./classifica-serie-a/2019-09-27_2019-09-30_5.csv\n",
      "./classifica-serie-a/2019-10-01_2019-10-06_6.csv\n",
      "./classifica-serie-a/2019-10-07_2019-10-21_7.csv\n",
      "./classifica-serie-a/2019-10-22_2019-10-27_8.csv\n",
      "./classifica-serie-a/2019-10-28_2019-10-31_9.csv\n",
      "./classifica-serie-a/2019-11-01_2019-11-04_10.csv\n",
      "./classifica-serie-a/2019-11-05_2019-11-10_11.csv\n",
      "./classifica-serie-a/2019-11-11_2019-11-25_12.csv\n",
      "./classifica-serie-a/2019-11-26_2019-12-02_13.csv\n",
      "./classifica-serie-a/2019-12-03_2019-12-08_15.csv\n",
      "./classifica-serie-a/2019-12-09_2019-12-16_15.csv\n",
      "./classifica-serie-a/2019-12-17_2019-12-22_16.csv\n",
      "./classifica-serie-a/2019-12-23_2020-01-06_17.csv\n",
      "./classifica-serie-a/2020-01-07_2020-01-13_18.csv\n",
      "./classifica-serie-a/2020-01-13_2020-01-20_19.csv\n",
      "./classifica-serie-a/2020-01-21_2020-01-26_20.csv\n",
      "./classifica-serie-a/2020-01-27_2020-02-03_21.csv\n",
      "./classifica-serie-a/2020-02-04_2020-02-09_22.csv\n",
      "./classifica-serie-a/2020-02-10_2020-02-17_23.csv\n",
      "./classifica-serie-a/2020-02-18_2020-02-23_24.csv\n",
      "./classifica-serie-a/2020-02-24_2020-03-09_25.csv\n",
      "Start date: 2019-08-27\n",
      "End date: 2019-09-01\n",
      "Classifica:\n",
      "Inter - 1\n",
      "Lazio - 2\n",
      "Napoli - 3\n",
      "Atalanta - 4\n",
      "Torino - 5\n",
      "Brescia - 6\n",
      "Juventus - 7\n",
      "Udinese - 8\n",
      "Roma - 9\n",
      "Genoa - 10\n",
      "Bologna - 11\n",
      "Verona - 12\n",
      "Fiorentina - 13\n",
      "SPAL - 14\n",
      "Sassuolo - 15\n",
      "Milan - 16\n",
      "Parma - 17\n",
      "Cagliari - 18\n",
      "Sampdoria - 19\n",
      "Lecce - 20\n",
      "\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "File `'../../UtilityPython.ipynb.py'` not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\pagan133\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\magics\\execution.py:696\u001b[0m, in \u001b[0;36mExecutionMagics.run\u001b[1;34m(self, parameter_s, runner, file_finder)\u001b[0m\n\u001b[0;32m    695\u001b[0m     fpath \u001b[38;5;241m=\u001b[39m arg_lst[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m--> 696\u001b[0m     filename \u001b[38;5;241m=\u001b[39m \u001b[43mfile_finder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    697\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\pagan133\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\utils\\path.py:91\u001b[0m, in \u001b[0;36mget_py_filename\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 91\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFile `\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m` not found.\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m name)\n",
      "\u001b[1;31mOSError\u001b[0m: File `'../../UtilityPython.ipynb.py'` not found.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m listdir\n\u001b[0;32m      7\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrun\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./classifica_serie_a.ipynb\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_line_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrun\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../../UtilityPython.ipynb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pagan133\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2305\u001b[0m, in \u001b[0;36mInteractiveShell.run_line_magic\u001b[1;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[0;32m   2303\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocal_ns\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_local_scope(stack_depth)\n\u001b[0;32m   2304\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[1;32m-> 2305\u001b[0m     result \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   2306\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\pagan133\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\magics\\execution.py:707\u001b[0m, in \u001b[0;36mExecutionMagics.run\u001b[1;34m(self, parameter_s, runner, file_finder)\u001b[0m\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnt\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m re\u001b[38;5;241m.\u001b[39mmatch(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m^\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.*\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m$\u001b[39m\u001b[38;5;124m\"\u001b[39m,fpath):\n\u001b[0;32m    706\u001b[0m         warn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFor Windows, use double quotes to wrap a filename: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124mun \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmypath\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mmyfile.py\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 707\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    708\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m    709\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m fpath \u001b[38;5;129;01min\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mmeta_path:\n",
      "\u001b[1;31mException\u001b[0m: File `'../../UtilityPython.ipynb.py'` not found."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date, timedelta, datetime\n",
    "import os.path\n",
    "from os import listdir\n",
    "\n",
    "%run ./classifica_serie_a.ipynb\n",
    "%run ../../UtilityPython.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_more_file(path):\n",
    "    filepaths = [f for f in listdir(path) if f.endswith('.csv')]\n",
    "    print(filepaths)\n",
    "    filepaths=list(map(lambda x : path + x,filepaths))\n",
    "    print(filepaths)\n",
    "\n",
    "    return pd.concat(map(lambda x : pd.read_csv(x, sep='~', keep_default_na=False, na_values=['_'],\n",
    "                                                error_bad_lines=False), filepaths))\n",
    "\n",
    "\n",
    "#Creo un nuovo dataframe con: (id, pro, data, max us, hour, type) \n",
    "def reshape_df(df, start_date, end_date):\n",
    "    sdate = datetime.strptime(start_date, '%d_%m_%Y')\n",
    "    edate = datetime.strptime(end_date, '%d_%m_%Y')\n",
    "    \n",
    "    delta = edate - sdate       # as timedelta\n",
    "    final = pd.DataFrame(columns = ['ID', 'PROVINCIA','DATA', 'CAPACITA', 'SOGLIA GIALLA', 'SOGLIA ROSSA', \n",
    "                                    'MAX_DS', 'INTERVALLO_MAX_DS', 'TIPO_GIORNO_DS'])\n",
    "    \n",
    "    for i in range(delta.days + 1):\n",
    "        day = sdate + timedelta(days=i)\n",
    "        \n",
    "        tmp = df.filter(regex= datetime.strftime(day, '%d_%m_%Y')).rename(\n",
    "            columns={'MAX DS ' + datetime.strftime(day, '%d_%m_%Y'):'MAX_DS',\n",
    "                     'INTERVALLO MAX DS ' + datetime.strftime(day, '%d_%m_%Y'):'INTERVALLO_MAX_DS',\n",
    "                     'TIPO GIORNO DS '+ datetime.strftime(day, '%d_%m_%Y'):'TIPO_GIORNO_DS'})\n",
    "        df_anag = df[['ID', 'PROVINCIA', 'CAPACITA', 'SOGLIA GIALLA', 'SOGLIA ROSSA',]]\n",
    "        tmp1 = pd.concat([df_anag, tmp], axis=1, sort=False)\n",
    "        tmp1.dropna(inplace=True)\n",
    "        tmp1['DATA'] = day\n",
    "        final=final.append(tmp1)\n",
    "        \n",
    "    return final\n",
    "\n",
    "#Setta il flag sulla logica dei team-partita\n",
    "def set_game_flag(team1, team2, s_d, squadra):\n",
    "    value = 0\n",
    "    if team1==squadra or team2==squadra:\n",
    "        value = 0.25 if s_d=='SKY' else 1\n",
    "    return value\n",
    "\n",
    "#Definisce il dataframe che associa le informazioni tra la partita e quale squadra ha giocato\n",
    "def calculate_match_vector(df):\n",
    "    match = pd.DataFrame(columns=['DATA', 'GAME_J', 'GAME_I', 'GAME_M', 'GAME_R', 'GAME_N'])\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        if row['data'] not in match['DATA'].unique():\n",
    "            match = match.append(pd.Series([row['data'], 0, 0, 0, 0, 0], index=match.columns ),\n",
    "                                 ignore_index=True)\n",
    "\n",
    "        match.loc[(match['DATA']==row['data']) & (match['GAME_J']==0),'GAME_J']=set_game_flag(\n",
    "            row['casa'], row['trasferta'] , row['diretta'], 'Juventus')\n",
    "        match.loc[(match['DATA']==row['data']) & (match['GAME_I']==0),'GAME_I']=set_game_flag(\n",
    "            row['casa'], row['trasferta'] , row['diretta'], 'Inter')\n",
    "        match.loc[(match['DATA']==row['data']) & (match['GAME_M']==0),'GAME_M']=set_game_flag(\n",
    "            row['casa'], row['trasferta'] , row['diretta'], 'Milan')\n",
    "        match.loc[(match['DATA']==row['data']) & (match['GAME_R']==0),'GAME_R']=set_game_flag(\n",
    "            row['casa'], row['trasferta'] , row['diretta'], 'Roma')\n",
    "        match.loc[(match['DATA']==row['data']) & (match['GAME_N']==0),'GAME_N']=set_game_flag(\n",
    "            row['casa'], row['trasferta'] , row['diretta'], 'Napoli')\n",
    "    return match\n",
    "\n",
    "def kpi_partita(data, juv, inter, mil, rm, na, g_j, g_i, g_m, g_r, g_n):\n",
    "    data_cal=data[:10]\n",
    "    '''\n",
    "    return (juv*g_j*get_posizione_classifica(data_cal, \"Juventus\")\n",
    "            + inter*g_i*get_posizione_classifica(data_cal, \"Inter\") \n",
    "            + mil*g_m*get_posizione_classifica(data_cal, \"Milan\") \n",
    "            + rm*g_r*get_posizione_classifica(data_cal, \"Roma\") \n",
    "            + na*g_n*get_posizione_classifica(data_cal, \"Napoli\"))#*match_hour(data)\n",
    "            '''\n",
    "    m_j = juv*g_j if g_j!=0 else juv*0.05 if (g_i!=0) | (g_m!=0) | (g_r!=0) | (g_n!=0) else 0\n",
    "    i_i = inter*g_i if g_i!=0 else inter*0.05 if (g_j!=0) | (g_m!=0) | (g_r!=0) | (g_n!=0) else 0\n",
    "    m_m = mil*g_m if g_m!=0 else mil*0.05 if (g_j!=0) | (g_i!=0) | (g_r!=0) | (g_n!=0) else 0\n",
    "    m_r = rm*g_r if g_r!=0 else rm*0.05 if (g_j!=0) | (g_i!=0) | (g_m!=0) | (g_n!=0) else 0\n",
    "    m_n = na*g_n if g_n!=0 else na*0.05 if (g_j!=0) | (g_i!=0) | (g_m!=0) | (g_r!=0)  else 0\n",
    "    return (m_j + i_i + m_m + m_r + m_n)#*match_hour(data)\n",
    "\n",
    "def match_hour(data):\n",
    "    data = datetime.strptime(data, \"%Y-%m-%d %H:%M:%S\")\n",
    "    return 2 if data.hour==21 else 0.5 if data.hour==18 else 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df: dataset del traffico\n",
    "#kpi: dataset dei kpi delle partite\n",
    "#prv: dataset con l'associazione regione-> provincia\n",
    "def pre_processing(df, kpi, prv):\n",
    "    #Eliminiamo gli apparati che hanno misure non valide\n",
    "    df=df[df['FLAG MISURA MANCANTE']!='Y']\n",
    "    \n",
    "    #Filtriamo le sole features che ci interessano, la misura dei picchi di traffico D e \n",
    "    #le anagrafiche che definiscono l'apparato con la provincia di appartenenza\n",
    "    df_new1 = df.filter(regex='MAX DS ')\n",
    "    df_new2 = df.filter(regex='TIPO GIORNO DS ')\n",
    "    df_anag = df[['APPARATO', 'CATEGORIA','ID_MISURE_ATTESTAZIONE', 'PROVINCIA', 'CAPACITA', 'SOGLIA GIALLA',\n",
    "                  'SOGLIA ROSSA', 'LINEE RES', 'LINEE BUS', 'LINEE WHO']]\n",
    "    result = pd.concat([df_anag, df_new1, df_new2], axis=1, sort=False)\n",
    "    \n",
    "    #Eliminiamo i dati di cui non abbiamo anagrafica (provincia o linee null) se presenti\n",
    "    result['LINEE RES'] = pd.to_numeric(result['LINEE RES'])\n",
    "    result['LINEE BUS'] = pd.to_numeric(result['LINEE BUS'])\n",
    "    result['LINEE WHO'] = pd.to_numeric(result['LINEE WHO'])\n",
    "    result.dropna(subset = ['LINEE RES', 'LINEE BUS', 'LINEE WHO', 'PROVINCIA'],inplace=True)\n",
    "\n",
    "    #Calcolo le percentuali per ottenere ogni apparato quante tipologie di linee possiede\n",
    "    result['LINEE RES PERC']=result['LINEE RES']/(result['LINEE RES']+result['LINEE BUS']+result['LINEE WHO'])\n",
    "    result['LINEE BUS PERC']=result['LINEE BUS']/(result['LINEE RES']+result['LINEE BUS']+result['LINEE WHO'])\n",
    "    result['LINEE WHO PERC']=result['LINEE WHO']/(result['LINEE RES']+result['LINEE BUS']+result['LINEE WHO'])\n",
    "    result.drop(['LINEE RES', 'LINEE BUS', 'LINEE WHO'],axis=1,inplace=True)\n",
    "    \n",
    "    #Filtriamo le linee che hanno percentuale di residenziali > 0.75\n",
    "    result_f=result[result['LINEE RES PERC']>=0.75]\n",
    "    result_f.drop(['LINEE RES PERC', 'LINEE BUS PERC', 'LINEE WHO PERC'],axis=1,inplace=True)\n",
    "    \n",
    "    #Creiamo ID\n",
    "    result_f['ID']=result_f['APPARATO']+result_f['CATEGORIA']+result_f['ID_MISURE_ATTESTAZIONE']\n",
    "    result_f.drop(['APPARATO', 'CATEGORIA', 'ID_MISURE_ATTESTAZIONE'],axis=1,inplace=True)\n",
    "    cols = ['ID']  + [col for col in result_f if col != 'ID']\n",
    "    result_f = result_f[cols]\n",
    "    \n",
    "    #Prendiamo gli apparati che risultano presenti su tutte le settimane contenute nel dataframe \n",
    "    tmp=result_f.groupby(['ID','PROVINCIA']).size().to_frame('number_of_week').sort_values(\n",
    "        ['number_of_week'], ascending=[False])\n",
    "    num_of_weeks = tmp.number_of_week.max()\n",
    "    join=pd.merge(result_f, tmp, on='ID', how='outer')\n",
    "    result_f=join[join.number_of_week==num_of_weeks]\n",
    "    result_f.drop(['number_of_week'],axis=1,inplace=True)\n",
    "    \n",
    "    #Richiamiamo la funzione reshape_df per splittare le colonne in righe\n",
    "    finale=reshape_df(result_f, result_f.columns[5].split(' ')[-1], result_f.columns[-1].split(' ')[-1])\n",
    "    finale['DATA']=finale['DATA'].dt.strftime('%d-%m-%Y')\n",
    "    #Scartiamo i valori zero di DS\n",
    "    finale=finale[finale.MAX_DS!='0']\n",
    "    \n",
    "    #Conversione tipi delle colonne\n",
    "    finale['MAX_DS'] = finale['MAX_DS'].str.replace(',', '.').astype(float)\n",
    "    finale['SOGLIA GIALLA'] = finale['SOGLIA GIALLA'].str.replace(',', '.').astype(float)\n",
    "    finale['SOGLIA ROSSA'] = finale['SOGLIA ROSSA'].str.replace(',', '.').astype(float)\n",
    "    finale['CAPACITA'] = finale['CAPACITA'].str.replace(',', '.').astype(float)\n",
    "    \n",
    "    #Ricaviamo le informazioni dalle colonne\n",
    "    finale.loc[finale['SOGLIA GIALLA']<finale['MAX_DS'], 'SOPRA_SOGLIA_GIALLA'] =1\n",
    "    finale.loc[finale['SOGLIA ROSSA']<finale['MAX_DS'], 'SOPRA_SOGLIA_ROSSA'] =1\n",
    "    finale.loc[finale['CAPACITA']<finale['MAX_DS'], 'SOPRA_CAPACITA'] =1\n",
    "    finale.fillna({'SOPRA_SOGLIA_GIALLA':0, 'SOPRA_SOGLIA_ROSSA':0, 'SOPRA_CAPACITA':0}, inplace=True)\n",
    "    finale.drop(['CAPACITA', 'SOGLIA GIALLA', 'SOGLIA ROSSA'], axis=1, inplace=True)\n",
    "    \n",
    "    finale.loc[:,'DATA'] = pd.to_datetime(finale.DATA + ' ' + finale.INTERVALLO_MAX_DS,\n",
    "                                          format=\"%d-%m-%Y %H:%M\")\n",
    "\n",
    "    traf_f=finale.groupby([pd.Grouper(key='DATA', freq='3H'), 'PROVINCIA', 'TIPO_GIORNO_DS']).agg({'MAX_DS': 'sum', \n",
    "                                             'SOPRA_SOGLIA_GIALLA' : 'sum',\n",
    "                                             'SOPRA_SOGLIA_ROSSA' : 'sum',\n",
    "                                             'SOPRA_CAPACITA' : 'sum'}).reset_index()\n",
    "    tmp = finale.groupby([pd.Grouper(key='DATA', freq='3H'), 'PROVINCIA', 'TIPO_GIORNO_DS']).size().to_frame(\n",
    "        'N_APPARATI').reset_index()\n",
    "    traf_f=pd.merge(traf_f, tmp, on=['DATA', 'PROVINCIA', 'TIPO_GIORNO_DS'], how='outer').reset_index(\n",
    "        drop=True)\n",
    "\n",
    "    #Convertiamo il tipo di giorno FER o FES in 0 o 1\n",
    "    traf_f.replace({'TIPO_GIORNO_DS': {'FER': 0, 'FES': 1}}, inplace=True)\n",
    "\n",
    "    #Aggiungiamo la regione\n",
    "    traf_f=pd.merge(traf_f, prv, on=['PROVINCIA'], how='left').reset_index(drop=True)\n",
    "    \n",
    "    #aggiungiamo il kpi\n",
    "    traf_f=pd.merge(traf_f, kpi, on=['REGIONE', 'DATA'], how='left').reset_index(drop=True)\n",
    "    \n",
    "    #Dove il KPI è null vuol dire che non c'è l'evento. Lo impostiamo a 0\n",
    "    traf_f=traf_f.fillna(0)\n",
    "    \n",
    "    if os.path.exists('./datasets/to_train.csv'):\n",
    "        traf_f.to_csv (r'./datasets/to_train.csv', index = None, header=False, mode='a')\n",
    "    else:\n",
    "        traf_f.to_csv (r'./datasets/to_train.csv', index = None, header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anag():\n",
    "    #Leggiamo il calendario della SERIE A (fix su NA)\n",
    "    cal = pd.read_csv('./datasets/calendario_completo_processed.csv')\n",
    "    cal.loc[cal['casa_prov']=='NP','casa_prov']='NA'\n",
    "    cal.loc[cal['trasferta_prov']=='NP','trasferta_prov']='NA'\n",
    "\n",
    "    #Calcoliamo le informazioni sul match\n",
    "    match=calculate_match_vector(cal)\n",
    "    \n",
    "    #Anagrafica prelevata da https://www.istat.it/it/archivio/6789\n",
    "    prv = pd.read_csv('./datasets/Elenco-comuni-italiani.csv', sep=';', encoding='latin-1',\n",
    "                      keep_default_na=False, na_values=['_'])\n",
    "    prv=prv.drop_duplicates(['Denominazione regione', 'Sigla automobilistica'])[\n",
    "    ['Denominazione regione', 'Sigla automobilistica']]\n",
    "    prv.rename(columns={'Denominazione regione':'REGIONE', \n",
    "                        'Sigla automobilistica':'PROVINCIA'\n",
    "                       }, \n",
    "                     inplace=True)\n",
    "\n",
    "    #Aggiungiamo le province mancanti\n",
    "    listOfSeries = [pd.Series(['Sardegna', 'OT'], index=prv.columns ) ,\n",
    "                    pd.Series(['Sardegna', 'OG'], index=prv.columns ) ,\n",
    "                    pd.Series(['Sardegna', 'CI'], index=prv.columns ),\n",
    "                    pd.Series(['Emilia-Romagna', 'SM'], index=prv.columns ), #Repubblica di San Marino inserita per comodità in Emilia\n",
    "                    pd.Series(['Sardegna', 'VS'], index=prv.columns ) ]\n",
    "    prv = prv.append(listOfSeries , ignore_index=True)\n",
    "    prv.reset_index(drop=True)\n",
    "    \n",
    "    #Anagrafica dei tifosi basata sui dati https://forum.termometropolitico.it/170882-percentuale-di-tifosi-per-squadra-italia-regione-per-regione.html\n",
    "    dictionary = {'REGIONE' : pd.Series(['Piemonte', 'Lombardia', \"Valle d'Aosta/Vallée d'Aoste\",\n",
    "                                         'Trentino-Alto Adige/Südtirol', 'Veneto', 'Friuli-Venezia Giulia',\n",
    "                                         'Liguria', 'Emilia-Romagna', 'Toscana', 'Umbria', 'Marche',\n",
    "                                         'Lazio', 'Abruzzo', 'Molise', 'Campania', 'Puglia', 'Basilicata',\n",
    "                                         'Calabria', 'Sicilia', 'Sardegna']),\n",
    "                  'JUVENTUS' : pd.Series([34.1, 22.5, 34.1, 33.4, 28.1, 24.9, 20.4, 24.2, 23.2, 23, 34,\n",
    "                                          16.3, 33.5, 32.9, 17.1, 28.8, 31.3, 29.6, 33.8, 23.4]),\n",
    "                  'INTER' : pd.Series([11.2, 26.9, 11.2, 17.1, 16.9, 12.8, 9.2, 15, 12.4, 16.1, 17.9, 6,\n",
    "                                       19.8, 19, 10, 14.4, 26, 16.4, 17.2, 13.2] ),\n",
    "                  'MILAN' : pd.Series([26.1, 10.9, 26.1, 24.3, 22.5, 23.5, 10.5, 18.7, 16.5, 18, 18.4, 6.5,\n",
    "                                       19.2, 22.2, 9.4, 19.9, 15.1, 14.1, 19.1, 12.6]), \n",
    "                  'ROMA' : pd.Series([1.5, np.NaN, 1.5, 3.9, np.NaN, np.NaN, 1.7, np.NaN, 2.5, 4.4, 3.3,\n",
    "                                      35.1, 5.8, 4.2, 2.9, 4.4, 3.8, 4.4, 3.8, 3.5]),\n",
    "                  'NAPOLI' : pd.Series([3.3, 4.6, 3.3, 4.5, 3.6, 2.8, 3.4, 3.3, 4.5, np.NaN, np.NaN, 4, 3.2,\n",
    "                                        7.9, 46.7, 4.7, 8.6, 4.5, 3.2, 2.7])}\n",
    "\n",
    "    anag_tifosi = pd.DataFrame(dictionary)\n",
    "    anag_tifosi=anag_tifosi.fillna(0)\n",
    "    anag_tifosi.JUVENTUS=anag_tifosi.JUVENTUS/100\n",
    "    anag_tifosi.INTER=anag_tifosi.INTER/100\n",
    "    anag_tifosi.MILAN=anag_tifosi.MILAN/100\n",
    "    anag_tifosi.ROMA=anag_tifosi.ROMA/100\n",
    "    anag_tifosi.NAPOLI=anag_tifosi.NAPOLI/100\n",
    "    \n",
    "    anag_tifosi['key']=1\n",
    "    match['key']=1\n",
    "    #Join tra l'anagrafica dei tifosi e quella dei match\n",
    "    df_anag = pd.merge(anag_tifosi, match, on='key')\n",
    "    \n",
    "    #Calcoliamo il valore di kpi per ogni partita e per ogni regione\n",
    "    kpi = pd.DataFrame(columns=['REGIONE', 'DATA', 'KPI'])\n",
    "\n",
    "    for index, row in df_anag.iterrows():\n",
    "        kpi = kpi.append(pd.Series([row['REGIONE'], row['DATA'], kpi_partita(row['DATA'],\n",
    "                                                                             row['JUVENTUS'], \n",
    "                                                                             row['INTER'],\n",
    "                                                                             row['MILAN'],\n",
    "                                                                             row['ROMA'], \n",
    "                                                                             row['NAPOLI'],\n",
    "                                                                             row['GAME_J'],\n",
    "                                                                             row['GAME_I'],\n",
    "                                                                             row['GAME_M'], \n",
    "                                                                             row['GAME_R'], \n",
    "                                                                             row['GAME_N'])]\n",
    "                                   , index=kpi.columns), ignore_index=True)\n",
    "        \n",
    "    kpi['DATA']=pd.to_datetime(kpi['DATA'])\n",
    "    kpi['DATA'] = pd.DatetimeIndex(kpi['DATA']) + timedelta(hours=1)\n",
    "\n",
    "    #Raggruppiamo i kpi su 3H \n",
    "    kpi_tmp=kpi.groupby(['REGIONE',pd.Grouper(key='DATA', freq='3H'), 'KPI']).size().to_frame('size').reset_index()\n",
    "    #I KPI si devono associare alle 3H precedenti, così da permettere la predizione sull'evento futuro\n",
    "    kpi_tmp['DATA'] = pd.DatetimeIndex(kpi_tmp['DATA']) - timedelta(hours=3)\n",
    "    kpi_tmp.drop('size',axis=1,inplace=True)\n",
    "    \n",
    "    return kpi_tmp, prv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['201941_Report_settimanale_DSLAM-OLT_07_10_2019_13_10_2019.csv', '201942_Report_settimanale_DSLAM-OLT_14_10_2019_20_10_2019.csv', '201943_Report_settimanale_DSLAM-OLT_21_10_2019_27_10_2019.csv', '201944_Report_settimanale_DSLAM-OLT_28_10_2019_03_11_2019.csv']\n",
      "['./Datasets/input/10/201941_Report_settimanale_DSLAM-OLT_07_10_2019_13_10_2019.csv', './Datasets/input/10/201942_Report_settimanale_DSLAM-OLT_14_10_2019_20_10_2019.csv', './Datasets/input/10/201943_Report_settimanale_DSLAM-OLT_21_10_2019_27_10_2019.csv', './Datasets/input/10/201944_Report_settimanale_DSLAM-OLT_28_10_2019_03_11_2019.csv']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ant.pagano\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:33: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['201945_Report_settimanale_DSLAM-OLT_04_11_2019_10_11_2019.csv', '201946_Report_settimanale_DSLAM-OLT_11_11_2019_17_11_2019.csv', '201947_Report_settimanale_DSLAM-OLT_18_11_2019_24_11_2019.csv', '201948_Report_settimanale_DSLAM-OLT_25_11_2019_01_12_2019.csv']\n",
      "['./Datasets/input/11/201945_Report_settimanale_DSLAM-OLT_04_11_2019_10_11_2019.csv', './Datasets/input/11/201946_Report_settimanale_DSLAM-OLT_11_11_2019_17_11_2019.csv', './Datasets/input/11/201947_Report_settimanale_DSLAM-OLT_18_11_2019_24_11_2019.csv', './Datasets/input/11/201948_Report_settimanale_DSLAM-OLT_25_11_2019_01_12_2019.csv']\n",
      "['201949_Report_settimanale_DSLAM-OLT_02_12_2019_08_12_2019.csv', '201950_Report_settimanale_DSLAM-OLT_09_12_2019_15_12_2019.csv', '201951_Report_settimanale_DSLAM-OLT_16_12_2019_22_12_2019.csv', '201952_Report_settimanale_DSLAM-OLT_23_12_2019_29_12_2019.csv']\n",
      "['./Datasets/input/12/201949_Report_settimanale_DSLAM-OLT_02_12_2019_08_12_2019.csv', './Datasets/input/12/201950_Report_settimanale_DSLAM-OLT_09_12_2019_15_12_2019.csv', './Datasets/input/12/201951_Report_settimanale_DSLAM-OLT_16_12_2019_22_12_2019.csv', './Datasets/input/12/201952_Report_settimanale_DSLAM-OLT_23_12_2019_29_12_2019.csv']\n",
      "['202001_Report_settimanale_DSLAM-OLT_30_12_2019_05_01_2020.csv', '202002_Report_settimanale_DSLAM-OLT_06_01_2020_12_01_2020.csv', '202003_Report_settimanale_DSLAM-OLT_13_01_2020_19_01_2020.csv', '202004_Report_settimanale_DSLAM-OLT_20_01_2020_26_01_2020.csv', '202005_Report_settimanale_DSLAM-OLT_27_01_2020_02_02_2020.csv']\n",
      "['./Datasets/input/20_01/202001_Report_settimanale_DSLAM-OLT_30_12_2019_05_01_2020.csv', './Datasets/input/20_01/202002_Report_settimanale_DSLAM-OLT_06_01_2020_12_01_2020.csv', './Datasets/input/20_01/202003_Report_settimanale_DSLAM-OLT_13_01_2020_19_01_2020.csv', './Datasets/input/20_01/202004_Report_settimanale_DSLAM-OLT_20_01_2020_26_01_2020.csv', './Datasets/input/20_01/202005_Report_settimanale_DSLAM-OLT_27_01_2020_02_02_2020.csv']\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists('./datasets/to_train.csv'):\n",
    "    os.remove('./datasets/to_train.csv')\n",
    "    \n",
    "kpi, prv = anag()\n",
    "\n",
    "#df=read_more_file('./Datasets/input/08/')\n",
    "#pre_processing(df, kpi, prv)\n",
    "#df=read_more_file('./Datasets/input/09/')\n",
    "#pre_processing(df, kpi, prv)\n",
    "df=read_more_file('./Datasets/input/10/')\n",
    "pre_processing(df, kpi, prv)\n",
    "df=read_more_file('./Datasets/input/11/')\n",
    "pre_processing(df, kpi, prv)\n",
    "df=read_more_file('./Datasets/input/12/')\n",
    "pre_processing(df, kpi, prv)\n",
    "df=read_more_file('./Datasets/input/20_01/')\n",
    "pre_processing(df, kpi, prv)\n",
    "#df=read_more_file('./Datasets/input/20_02/')\n",
    "#pre_processing(df, kpi, prv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
