{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cccd1dc-0148-4886-96fb-c5924704e241",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Wine Quality e Type\n",
    "Questo notebook illustra il processo di costruzione, addestramento e valutazione di un modello di machine learning per la previsione della qualità del vino. In particolare si addestrerà una rete neurale multi input (N input pari al numero di colonne), multi output (2 colonne target, Qualità - valore numerico che indica la qualità del vino con una scala 1-10, e Tipo - variabile binaria rosso o bianco)\n",
    "Il focus principale è la creazione di un modello di deep learning per prevedere la qualità del vino basandosi su varie caratteristiche chimiche."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fafb69f2-b1a3-4e98-b448-25df608b596e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "- Importazione delle librerie necessarie, tra cui TensorFlow per la costruzione del modello, pandas e seaborn per la manipolazione e la visualizzazione dei dati, e scikit-learn per la divisione del dataset\n",
    "- Lettura del dataset\n",
    "- Visualizzazione delle qualità uniche del vino presenti nel dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "659a6472-1ff0-4777-8e26-2d5cc7f19931",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pagan133\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\computation\\expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    }
   ],
   "source": [
    "#Importing the libraries. \n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import tensorboard\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.python.keras.utils.vis_utils import plot_model\n",
    "\n",
    "from scipy.stats import norm\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2c741b9-e0ce-49af-beec-2c1cf351c0e5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>white</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.36</td>\n",
       "      <td>20.7</td>\n",
       "      <td>0.045</td>\n",
       "      <td>45.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>1.0010</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>8.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>white</td>\n",
       "      <td>6.3</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.049</td>\n",
       "      <td>14.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.9940</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.49</td>\n",
       "      <td>9.5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>white</td>\n",
       "      <td>8.1</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.40</td>\n",
       "      <td>6.9</td>\n",
       "      <td>0.050</td>\n",
       "      <td>30.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.9951</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.44</td>\n",
       "      <td>10.1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>white</td>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>white</td>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    type  fixed acidity  volatile acidity  citric acid  residual sugar  \\\n",
       "0  white            7.0              0.27         0.36            20.7   \n",
       "1  white            6.3              0.30         0.34             1.6   \n",
       "2  white            8.1              0.28         0.40             6.9   \n",
       "3  white            7.2              0.23         0.32             8.5   \n",
       "4  white            7.2              0.23         0.32             8.5   \n",
       "\n",
       "   chlorides  free sulfur dioxide  total sulfur dioxide  density    pH  \\\n",
       "0      0.045                 45.0                 170.0   1.0010  3.00   \n",
       "1      0.049                 14.0                 132.0   0.9940  3.30   \n",
       "2      0.050                 30.0                  97.0   0.9951  3.26   \n",
       "3      0.058                 47.0                 186.0   0.9956  3.19   \n",
       "4      0.058                 47.0                 186.0   0.9956  3.19   \n",
       "\n",
       "   sulphates  alcohol  quality  \n",
       "0       0.45      8.8        6  \n",
       "1       0.49      9.5        6  \n",
       "2       0.44     10.1        6  \n",
       "3       0.40      9.9        6  \n",
       "4       0.40      9.9        6  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./dataset/winequalityN.csv\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72210919-6740-4167-91ce-96a557f33545",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 5, 7, 8, 4, 3, 9], dtype=int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "distinct_qualities = df['quality'].unique()\n",
    "display(distinct_qualities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f66fb76f-1aa5-44d8-8373-564eec7cafab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Converting string column to categorical numeric. \n",
    "df['type'] = df['type'].apply(lambda x:0 if (x == 'white') else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03a76732-aaca-48aa-9357-9e24d1771915",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fixed acidity          10\n",
       "pH                      9\n",
       "volatile acidity        8\n",
       "sulphates               4\n",
       "citric acid             3\n",
       "residual sugar          2\n",
       "chlorides               2\n",
       "type                    0\n",
       "free sulfur dioxide     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get the numbers of Null values in columns, in descending order. \n",
    "df.isna().sum().sort_values(ascending=False).head(9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d47781c6-d735-4b25-9606-5105c7e37bc4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "Viene creato un nuovo dataset eliminando le righe che contengono valori nulli o vuoti. Questa operazione è necessaria per assicurarsi che il dataset sia pulito e non contenga dati mancanti, in modo da poter eseguire analisi e modellazione accurata.\n",
    "\n",
    "Viene controllato se ci sono ancora valori nulli nel dataset. Viene calcolato il numero di valori nulli per ciascuna colonna, ordinati in ordine decrescente e vengono mostrati solo i primi 8. Questa operazione è utile per identificare eventuali colonne con valori mancanti e valutare l'impatto che potrebbero avere sull'analisi o sul modello.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de5fb340-bfe4-4d77-8a78-1963aaf3870e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Creating the new dataset without null, or empty, values. \n",
    "df = df.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e00401d-f985-4fb8-948a-f4856f68d765",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "type                    0\n",
       "fixed acidity           0\n",
       "volatile acidity        0\n",
       "citric acid             0\n",
       "residual sugar          0\n",
       "chlorides               0\n",
       "free sulfur dioxide     0\n",
       "total sulfur dioxide    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check if we have more nulls. \n",
    "df.isna().sum().sort_values(ascending=False).head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0542157-188d-4978-a230-f806a4ee6bb8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "Viene creato il set di dati di addestramento e di test. Il 80% dei dati viene utilizzato per l'addestramento e il 20% per il test. Questa divisione è comune per valutare le prestazioni del modello su dati non visti durante l'addestramento.\n",
    "\n",
    "Viene quindi creato un ulteriore set di dati di addestramento e di validazione. Il 20% dei dati di addestramento viene utilizzato per la validazione. Questa divisione è utile per valutare le prestazioni del modello durante l'addestramento e per evitare l'overfitting.\n",
    "\n",
    "Viene definita una funzione che estrae le etichette \"type\" e \"quality\" dal dataset. Queste etichette vengono rimosse dal dataset originale e convertite in array numpy. Questa operazione è necessaria per preparare le etichette da utilizzare come output del modello di machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53e3549e-42a6-4cad-a38f-ce033a99dced",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Create the train and test data. 80% tain and 20 data. \n",
    "train, test = train_test_split(df, test_size=0.2, random_state = 1)\n",
    "\n",
    "#From the train Data we are going to get a 20% more to create the validation data. \n",
    "train, val = train_test_split(train, test_size=0.2, random_state = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "537d9b21-02be-4ac7-a311-b7a754156ac9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#With this function we got the labels *type* and *quality*,  ready to pass to the Model. \n",
    "def get_labels(df):\n",
    "    type_wine = df.pop('type')\n",
    "    type_wine = np.array(type_wine)\n",
    "    quality = df.pop('quality')\n",
    "    quality = np.array(quality)\n",
    "    return (quality, type_wine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39ca9afe-fbd3-47ea-81c7-940e0bc41f6c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Getting the labels for train, test and validate. \n",
    "train_y = get_labels(train)\n",
    "test_y = get_labels(test)\n",
    "val_y = get_labels(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6316b901-7b9a-42a1-b8b3-3f62083ed3f2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fixed acidity</th>\n",
       "      <td>4136.0</td>\n",
       "      <td>7.196905</td>\n",
       "      <td>1.296497</td>\n",
       "      <td>3.80000</td>\n",
       "      <td>6.4000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.62500</td>\n",
       "      <td>15.90000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>volatile acidity</th>\n",
       "      <td>4136.0</td>\n",
       "      <td>0.337378</td>\n",
       "      <td>0.163193</td>\n",
       "      <td>0.08000</td>\n",
       "      <td>0.2200</td>\n",
       "      <td>0.290000</td>\n",
       "      <td>0.40000</td>\n",
       "      <td>1.58000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>citric acid</th>\n",
       "      <td>4136.0</td>\n",
       "      <td>0.319108</td>\n",
       "      <td>0.144535</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.310000</td>\n",
       "      <td>0.39000</td>\n",
       "      <td>1.66000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>residual sugar</th>\n",
       "      <td>4136.0</td>\n",
       "      <td>5.510638</td>\n",
       "      <td>4.812892</td>\n",
       "      <td>0.60000</td>\n",
       "      <td>1.8000</td>\n",
       "      <td>3.200000</td>\n",
       "      <td>8.20000</td>\n",
       "      <td>65.80000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chlorides</th>\n",
       "      <td>4136.0</td>\n",
       "      <td>0.055627</td>\n",
       "      <td>0.035004</td>\n",
       "      <td>0.01200</td>\n",
       "      <td>0.0380</td>\n",
       "      <td>0.047000</td>\n",
       "      <td>0.06400</td>\n",
       "      <td>0.61100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <td>4136.0</td>\n",
       "      <td>30.732834</td>\n",
       "      <td>17.917700</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>17.0000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>42.00000</td>\n",
       "      <td>289.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <td>4136.0</td>\n",
       "      <td>116.689313</td>\n",
       "      <td>56.244277</td>\n",
       "      <td>7.00000</td>\n",
       "      <td>80.0000</td>\n",
       "      <td>119.000000</td>\n",
       "      <td>156.00000</td>\n",
       "      <td>440.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>density</th>\n",
       "      <td>4136.0</td>\n",
       "      <td>0.994708</td>\n",
       "      <td>0.003034</td>\n",
       "      <td>0.98713</td>\n",
       "      <td>0.9923</td>\n",
       "      <td>0.994885</td>\n",
       "      <td>0.99704</td>\n",
       "      <td>1.03898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pH</th>\n",
       "      <td>4136.0</td>\n",
       "      <td>3.218873</td>\n",
       "      <td>0.160356</td>\n",
       "      <td>2.72000</td>\n",
       "      <td>3.1100</td>\n",
       "      <td>3.210000</td>\n",
       "      <td>3.32000</td>\n",
       "      <td>4.01000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sulphates</th>\n",
       "      <td>4136.0</td>\n",
       "      <td>0.530771</td>\n",
       "      <td>0.148288</td>\n",
       "      <td>0.23000</td>\n",
       "      <td>0.4300</td>\n",
       "      <td>0.510000</td>\n",
       "      <td>0.60000</td>\n",
       "      <td>2.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alcohol</th>\n",
       "      <td>4136.0</td>\n",
       "      <td>10.480809</td>\n",
       "      <td>1.190722</td>\n",
       "      <td>8.00000</td>\n",
       "      <td>9.5000</td>\n",
       "      <td>10.300000</td>\n",
       "      <td>11.30000</td>\n",
       "      <td>14.90000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       count        mean  ...        75%        max\n",
       "fixed acidity         4136.0    7.196905  ...    7.62500   15.90000\n",
       "volatile acidity      4136.0    0.337378  ...    0.40000    1.58000\n",
       "citric acid           4136.0    0.319108  ...    0.39000    1.66000\n",
       "residual sugar        4136.0    5.510638  ...    8.20000   65.80000\n",
       "chlorides             4136.0    0.055627  ...    0.06400    0.61100\n",
       "free sulfur dioxide   4136.0   30.732834  ...   42.00000  289.00000\n",
       "total sulfur dioxide  4136.0  116.689313  ...  156.00000  440.00000\n",
       "density               4136.0    0.994708  ...    0.99704    1.03898\n",
       "pH                    4136.0    3.218873  ...    3.32000    4.01000\n",
       "sulphates             4136.0    0.530771  ...    0.60000    2.00000\n",
       "alcohol               4136.0   10.480809  ...   11.30000   14.90000\n",
       "\n",
       "[11 rows x 8 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get Mean and std but only from training data. \n",
    "train_stats = train.describe()\n",
    "train_stats = train_stats.transpose()\n",
    "train_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0960030-9804-4dc4-9d6f-69db43f89734",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Normalize the data, but with the mean and std of only train data. \n",
    "def scale_data(df):\n",
    "    return (df - train_stats['mean']) / train_stats['std']\n",
    "\n",
    "#Scaling the 3 datasets. \n",
    "train_X = scale_data(train)\n",
    "test_X = scale_data(test)\n",
    "val_X = scale_data(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3aca4c89-5230-4dcc-90dd-07d78c79e052",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "La rete neurale implementata è un modello di regressione multi-output. È stata progettata in questo modo per affrontare un problema di previsione di due variabili di output: il tipo di vino (variabile binaria) e la qualità del vino (variabile continua).\n",
    "\n",
    "Il modello è composto da un input layer che specifica la forma dei dati in ingresso, seguito da due dense layers con funzione di attivazione ReLU. Questi layer sono comuni a entrambe le variabili di output.\n",
    "\n",
    "Per la variabile di output \"tipo di vino\", è stato aggiunto un output layer con funzione di attivazione sigmoide. Questo layer restituisce una probabilità che il vino sia di un determinato tipo.\n",
    "\n",
    "Per la variabile di output \"qualità del vino\", è stato aggiunto un dense layer aggiuntivo chiamato \"quality_layer\" con funzione di attivazione ReLU. Questo layer introduce una diversificazione nel modello.\n",
    "\n",
    "Infine, sono stati definiti due output layers: \"y_q_layer\" per la variabile di output \"qualità del vino\" e \"y_t_layer\" per la variabile di output \"tipo di vino\".\n",
    "\n",
    "Il modello è stato compilato utilizzando l'ottimizzatore Adam e le seguenti funzioni di loss e metriche:\n",
    "- Per la variabile di output \"tipo di vino\", è stata utilizzata la funzione di loss \"binary_crossentropy\" e la metrica \"accuracy\".\n",
    "- Per la variabile di output \"qualità del vino\", è stata utilizzata la funzione di loss \"mse\" (Mean Squared Error) e la metrica \"RootMeanSquaredError\".\n",
    "\n",
    "Questo tipo di rete neurale è stato scelto per gestire un problema di previsione multi-output, in cui si desidera prevedere più di una variabile di output. L'aggiunta del dense layer \"quality_layer\" permette di introdurre una diversificazione nel modello, consentendo di catturare relazioni più complesse tra le variabili di input e l'output \"qualità del vino\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed89d8c2-4b09-4231-a7ad-7628b51063e3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Un hidden layer, in particolare di tipo dense, è uno strato di neuroni in una rete neurale che riceve input dai neuroni dello strato precedente e invia output ai neuroni dello strato successivo. Questo strato è chiamato \"hidden\" perché i suoi neuroni non sono direttamente visibili all'esterno della rete neurale.\n",
    "\n",
    "Nel contesto di una rete neurale densamente connessa (dense neural network), ogni neurone in uno strato hidden dense è connesso a tutti i neuroni dello strato precedente e a tutti i neuroni dello strato successivo. Questo significa che ogni neurone riceve input da tutti i neuroni dello strato precedente e invia output a tutti i neuroni dello strato successivo.\n",
    "\n",
    "La funzione di attivazione, in particolare la funzione ReLU (Rectified Linear Unit), è una funzione matematica che viene applicata ai valori di output dei neuroni in uno strato hidden dense. La funzione ReLU è definita come f(x) = max(0, x), dove x è il valore di input del neurone. Questa funzione è non lineare e introduce la non linearità nella rete neurale.\n",
    "\n",
    "La funzione ReLU è ampiamente utilizzata nelle reti neurali perché è semplice da calcolare e risolve il problema della scomparsa del gradiente. Inoltre, la funzione ReLU è in grado di approssimare funzioni complesse e di introdurre la capacità di apprendimento non lineare nella rete neurale.\n",
    "\n",
    "In sintesi, uno hidden layer di tipo dense è uno strato di neuroni in una rete neurale che riceve input dai neuroni dello strato precedente e invia output ai neuroni dello strato successivo. La funzione di attivazione ReLU viene applicata ai valori di output dei neuroni in questo strato per introdurre non linearità nella rete neurale e consentire l'apprendimento di relazioni complesse tra i dati di input e l'output desiderato."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86f40ceb-7172-4607-8f83-3bd58d3bb78c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "Le due funzioni di loss utilizzate nel modello sono \"binary_crossentropy\" e \"mse\" (Mean Squared Error).\n",
    "\n",
    "La funzione di loss \"binary_crossentropy\" viene utilizzata per la variabile di output \"tipo di vino\", che è una variabile binaria. Questa funzione di loss calcola l'errore tra la probabilità predetta dal modello per il vino di un determinato tipo e il valore reale della variabile di output. La formula della funzione di loss \"binary_crossentropy\" è:\n",
    "\n",
    "loss = - (y_true * log(y_pred) + (1 - y_true) * log(1 - y_pred))\n",
    "\n",
    "Dove:\n",
    "- y_true è il valore reale della variabile di output (0 o 1)\n",
    "- y_pred è la probabilità predetta dal modello per il vino di un determinato tipo\n",
    "\n",
    "La funzione di loss \"mse\" viene utilizzata per la variabile di output \"qualità del vino\", che è una variabile continua. Questa funzione di loss calcola l'errore quadratico medio tra il valore predetto dal modello per la qualità del vino e il valore reale della variabile di output. La formula della funzione di loss \"mse\" è:\n",
    "\n",
    "loss = (1/n) * sum((y_true - y_pred)^2)\n",
    "\n",
    "Dove:\n",
    "- y_true è il valore reale della variabile di output\n",
    "- y_pred è il valore predetto dal modello per la qualità del vino\n",
    "- n è il numero di campioni nel dataset\n",
    "\n",
    "Le due funzioni di metrica utilizzate nel modello sono \"accuracy\" e \"RootMeanSquaredError\".\n",
    "\n",
    "La metrica \"accuracy\" viene utilizzata per valutare le prestazioni del modello nella previsione del tipo di vino. Questa metrica calcola la percentuale di predizioni corrette rispetto al numero totale di predizioni. La formula della metrica \"accuracy\" è:\n",
    "\n",
    "accuracy = (numero di predizioni corrette) / (numero totale di predizioni)\n",
    "\n",
    "La metrica \"RootMeanSquaredError\" viene utilizzata per valutare le prestazioni del modello nella previsione della qualità del vino. Questa metrica calcola la radice quadrata dell'errore quadratico medio tra il valore predetto dal modello e il valore reale della variabile di output. La formula della metrica \"RootMeanSquaredError\" è:\n",
    "\n",
    "RMSE = sqrt((1/n) * sum((y_true - y_pred)^2))\n",
    "\n",
    "Dove:\n",
    "- y_true è il valore reale della variabile di output\n",
    "- y_pred è il valore predetto dal modello per la qualità del vino\n",
    "- n è il numero di campioni nel dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61ed926f-8953-4d17-99af-23e9ac0bcdb6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Start with the input layer, where we must indicate the shape of the Data passed to the model. \n",
    "inputs = tf.keras.layers.Input(shape=(11,))\n",
    "\n",
    "#Add dense layers to the input layer. These layers are commom to both predicted variables. \n",
    "x = Dense(units=32, activation='relu')(inputs)\n",
    "x = Dense(units=32, activation='relu')(x)\n",
    "\n",
    "#Add the output layer for the Wine type using Sigmoid activation. \n",
    "y_t_layer = Dense(units = 1, activation='sigmoid', name='y_t_layer')(x)\n",
    "\n",
    "#Here we diversificate the model adding a new Dense layer to the Base layers (x)\n",
    "quality_layer=Dense(units=64, name='quality_layer', activation='relu')(x)\n",
    "\n",
    "#The output layer for the quality wine variable. It's added below the Dense Layer: quality_layer \n",
    "y_q_layer = Dense(units=1, name='y_q_layer')(quality_layer)\n",
    "\n",
    "#The Model is created indicating the inputs and outputs. \n",
    "#We have only one Input, but we can create models with multiple inputs. \n",
    "#The name in outputs is the same of the variables, and the internal name of the layer. \n",
    "model = Model(inputs=inputs, outputs=[y_q_layer, y_t_layer])\n",
    "\n",
    "#I tested two optimizers and choosed Adam, but feel free to test yourself. \n",
    "#optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.0001)\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "#To compile the model we use two dictionaries, to indicate the loss functions and metrics \n",
    "#for each output layer. Note that the name of the layer must be the same than the \n",
    "#internal name of the layer. \n",
    "model.compile(optimizer=optimizer, \n",
    "              loss = {'y_t_layer' : 'binary_crossentropy', \n",
    "                      'y_q_layer' : 'mse'\n",
    "                     },\n",
    "              metrics = {'y_t_layer' : 'accuracy', \n",
    "                         'y_q_layer': tf.keras.metrics.RootMeanSquaredError()\n",
    "                       }\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9796395c-f133-472c-9268-5c5af2021e90",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "Le summary del modello ci forniscono informazioni sulle diverse layer del modello, inclusi il tipo di layer, la forma dell'output di ogni layer e il numero di parametri di ogni layer.\n",
    "\n",
    "Nel caso specifico del modello in questione, la summary ci indica che ci sono 4 layer nel modello:\n",
    "\n",
    "1. Un layer di input con una forma di (None, 11), che indica che il modello accetta un input di dimensione (batch_size, 11).\n",
    "2. Due layer densi con 32 unità ciascuno e funzione di attivazione ReLU.\n",
    "3. Un layer di output per la variabile \"tipo di vino\" con 1 unità e funzione di attivazione sigmoide.\n",
    "4. Un layer denso con 64 unità e funzione di attivazione ReLU.\n",
    "5. Un layer di output per la variabile \"qualità del vino\" con 1 unità.\n",
    "\n",
    "Il numero di parametri di ogni layer è determinato dalla formula: (numero_di_unità_del_layer_precedente + 1) * numero_di_unità_del_layer_corrente. Il \"+1\" è dovuto al bias.\n",
    "\n",
    "Quindi, nel caso del primo layer denso, abbiamo (11 + 1) * 32 = 384 parametri. Nel caso del secondo layer denso, abbiamo (32 + 1) * 32 = 1056 parametri. Nel caso del layer di output per la variabile \"tipo di vino\", abbiamo (32 + 1) * 1 = 33 parametri. Nel caso del layer denso successivo, abbiamo (32 + 1) * 64 = 2112 parametri. Infine, nel caso del layer di output per la variabile \"qualità del vino\", abbiamo (64 + 1) * 1 = 65 parametri.\n",
    "\n",
    "Questi parametri rappresentano i pesi e i bias delle connessioni tra i neuroni dei diversi layer del modello. Sono questi parametri che vengono addestrati durante il processo di addestramento del modello per cercare di minimizzare la funzione di loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "500e6ed6-8014-4e75-adbe-c5d7931f4b5b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 11)]                 0         []                            \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 32)                   384       ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 32)                   1056      ['dense[0][0]']               \n",
      "                                                                                                  \n",
      " quality_layer (Dense)       (None, 64)                   2112      ['dense_1[0][0]']             \n",
      "                                                                                                  \n",
      " y_q_layer (Dense)           (None, 1)                    65        ['quality_layer[0][0]']       \n",
      "                                                                                                  \n",
      " y_t_layer (Dense)           (None, 1)                    33        ['dense_1[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3650 (14.26 KB)\n",
      "Trainable params: 3650 (14.26 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8910bb38-ec20-4194-acac-47851630c1af",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "La funzione `fit` del modello viene utilizzata per addestrare il modello sui dati di addestramento. Durante il processo di addestramento, il modello cerca di apprendere i pesi e i bias delle connessioni tra i neuroni dei diversi layer del modello in modo da minimizzare la funzione di loss.\n",
    "\n",
    "Gli step di addestramento in output ci forniscono informazioni sul processo di addestramento del modello. Questi step includono:\n",
    "\n",
    "- `loss`: il valore della funzione di loss ottenuto dopo l'addestramento del modello sui dati di validazione. Un valore di loss più basso indica una migliore performance del modello.\n",
    "- `wine_quality_loss`: il valore della funzione di loss specifica per la variabile \"qualità del vino\" ottenuto dopo l'addestramento del modello sui dati di validazione.\n",
    "- `wine_type_loss`: il valore della funzione di loss specifica per la variabile \"tipo di vino\" ottenuto dopo l'addestramento del modello sui dati di validazione.\n",
    "- `wine_quality_rmse`: la radice dell'errore quadratico medio (RMSE) per la variabile \"qualità del vino\" ottenuto dopo l'addestramento del modello sui dati di validazione. Un valore di RMSE più basso indica una migliore performance del modello.\n",
    "- `wine_type_accuracy`: l'accuratezza del modello nella previsione della variabile \"tipo di vino\" ottenuta dopo l'addestramento del modello sui dati di validazione. Un valore di accuratezza più alto indica una migliore performance del modello.\n",
    "\n",
    "Questi output ci permettono di valutare la performance del modello dopo l'addestramento e di confrontarla con i risultati ottenuti durante l'addestramento stesso. In questo modo possiamo capire se il modello sta imparando correttamente dai dati di addestramento e se sta generalizzando bene su nuovi dati."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffc19b45-15e7-4d15-a7ed-dba453899eac",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "La backpropagation è un algoritmo utilizzato durante la fase di addestramento di una rete neurale per calcolare il gradiente della funzione di loss rispetto ai pesi e ai bias del modello. Questo calcolo del gradiente consente di aggiornare i pesi e i bias in modo da minimizzare la funzione di loss durante l'ottimizzazione del modello.\n",
    "\n",
    "Durante la fase di forward pass, i dati di addestramento vengono propagati attraverso il modello per ottenere le previsioni del modello. Successivamente, durante la fase di backward pass, il gradiente della funzione di loss viene calcolato rispetto ai pesi e ai bias del modello utilizzando la regola della catena. Questo gradiente viene quindi utilizzato per aggiornare i pesi e i bias del modello utilizzando un algoritmo di ottimizzazione come la discesa del gradiente.\n",
    "\n",
    "La backpropagation è utile per la fase di fit del modello perché consente di calcolare il gradiente della funzione di loss rispetto ai pesi e ai bias del modello. Questo gradiente viene utilizzato per aggiornare i pesi e i bias del modello in modo da migliorare la performance del modello durante l'addestramento. Senza la backpropagation, sarebbe molto più difficile ottimizzare i pesi e i bias del modello e ottenere una buona performance di addestramento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a4d4825-a388-476c-bd08-643a9a5cda0c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "\r  1/130 [..............................] - ETA: 2:13 - loss: 35.5470 - y_q_layer_loss: 34.8455 - y_t_layer_loss: 0.7015 - y_q_layer_root_mean_squared_error: 5.9030 - y_t_layer_accuracy: 0.6875WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0013s vs `on_train_batch_end` time: 0.0035s). Check your callbacks.\n",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 28/130 [=====>........................] - ETA: 0s - loss: 27.8150 - y_q_layer_loss: 27.1422 - y_t_layer_loss: 0.6729 - y_q_layer_root_mean_squared_error: 5.2098 - y_t_layer_accuracy: 0.7478  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 54/130 [===========>..................] - ETA: 0s - loss: 18.7413 - y_q_layer_loss: 18.0844 - y_t_layer_loss: 0.6568 - y_q_layer_root_mean_squared_error: 4.2526 - y_t_layer_accuracy: 0.7477\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 83/130 [==================>...........] - ETA: 0s - loss: 13.6765 - y_q_layer_loss: 13.0681 - y_t_layer_loss: 0.6084 - y_q_layer_root_mean_squared_error: 3.6150 - y_t_layer_accuracy: 0.7549\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r111/130 [========================>.....] - ETA: 0s - loss: 11.0033 - y_q_layer_loss: 10.4393 - y_t_layer_loss: 0.5640 - y_q_layer_root_mean_squared_error: 3.2310 - y_t_layer_accuracy: 0.7562\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r130/130 [==============================] - 2s 8ms/step - loss: 9.9758 - y_q_layer_loss: 9.4329 - y_t_layer_loss: 0.5429 - y_q_layer_root_mean_squared_error: 3.0713 - y_t_layer_accuracy: 0.7611 - val_loss: 2.5025 - val_y_q_layer_loss: 2.1075 - val_y_t_layer_loss: 0.3950 - val_y_q_layer_root_mean_squared_error: 1.4517 - val_y_t_layer_accuracy: 0.8240\n",
      "Epoch 2/40\n",
      "\r  1/130 [..............................] - ETA: 0s - loss: 2.5301 - y_q_layer_loss: 2.0673 - y_t_layer_loss: 0.4628 - y_q_layer_root_mean_squared_error: 1.4378 - y_t_layer_accuracy: 0.7188\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 32/130 [======>.......................] - ETA: 0s - loss: 2.8033 - y_q_layer_loss: 2.4500 - y_t_layer_loss: 0.3533 - y_q_layer_root_mean_squared_error: 1.5652 - y_t_layer_accuracy: 0.8750\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 63/130 [=============>................] - ETA: 0s - loss: 2.3615 - y_q_layer_loss: 2.0289 - y_t_layer_loss: 0.3327 - y_q_layer_root_mean_squared_error: 1.4244 - y_t_layer_accuracy: 0.8844\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 93/130 [====================>.........] - ETA: 0s - loss: 2.1879 - y_q_layer_loss: 1.8798 - y_t_layer_loss: 0.3081 - y_q_layer_root_mean_squared_error: 1.3711 - y_t_layer_accuracy: 0.9009\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r123/130 [===========================>..] - ETA: 0s - loss: 2.0761 - y_q_layer_loss: 1.7867 - y_t_layer_loss: 0.2894 - y_q_layer_root_mean_squared_error: 1.3367 - y_t_layer_accuracy: 0.9106\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r130/130 [==============================] - 0s 2ms/step - loss: 2.0554 - y_q_layer_loss: 1.7703 - y_t_layer_loss: 0.2851 - y_q_layer_root_mean_squared_error: 1.3305 - y_t_layer_accuracy: 0.9127 - val_loss: 1.5229 - val_y_q_layer_loss: 1.3099 - val_y_t_layer_loss: 0.2130 - val_y_q_layer_root_mean_squared_error: 1.1445 - val_y_t_layer_accuracy: 0.9613\n",
      "Epoch 3/40\n",
      "\r  1/130 [..............................] - ETA: 0s - loss: 1.5974 - y_q_layer_loss: 1.3656 - y_t_layer_loss: 0.2318 - y_q_layer_root_mean_squared_error: 1.1686 - y_t_layer_accuracy: 0.9375\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 32/130 [======>.......................] - ETA: 0s - loss: 1.3521 - y_q_layer_loss: 1.1586 - y_t_layer_loss: 0.1934 - y_q_layer_root_mean_squared_error: 1.0764 - y_t_layer_accuracy: 0.9658\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 62/130 [=============>................] - ETA: 0s - loss: 1.3404 - y_q_layer_loss: 1.1640 - y_t_layer_loss: 0.1763 - y_q_layer_root_mean_squared_error: 1.0789 - y_t_layer_accuracy: 0.9728\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 92/130 [====================>.........] - ETA: 0s - loss: 1.3434 - y_q_layer_loss: 1.1743 - y_t_layer_loss: 0.1691 - y_q_layer_root_mean_squared_error: 1.0836 - y_t_layer_accuracy: 0.9735\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r123/130 [===========================>..] - ETA: 0s - loss: 1.3402 - y_q_layer_loss: 1.1789 - y_t_layer_loss: 0.1613 - y_q_layer_root_mean_squared_error: 1.0858 - y_t_layer_accuracy: 0.9756\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r130/130 [==============================] - 0s 2ms/step - loss: 1.3358 - y_q_layer_loss: 1.1765 - y_t_layer_loss: 0.1593 - y_q_layer_root_mean_squared_error: 1.0847 - y_t_layer_accuracy: 0.9765 - val_loss: 1.0441 - val_y_q_layer_loss: 0.9180 - val_y_t_layer_loss: 0.1261 - val_y_q_layer_root_mean_squared_error: 0.9581 - val_y_t_layer_accuracy: 0.9826\n",
      "Epoch 4/40\n",
      "\r  1/130 [..............................] - ETA: 0s - loss: 1.6346 - y_q_layer_loss: 1.5359 - y_t_layer_loss: 0.0987 - y_q_layer_root_mean_squared_error: 1.2393 - y_t_layer_accuracy: 1.0000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 30/130 [=====>........................] - ETA: 0s - loss: 1.0439 - y_q_layer_loss: 0.9281 - y_t_layer_loss: 0.1157 - y_q_layer_root_mean_squared_error: 0.9634 - y_t_layer_accuracy: 0.9885\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 61/130 [=============>................] - ETA: 0s - loss: 0.9935 - y_q_layer_loss: 0.8846 - y_t_layer_loss: 0.1088 - y_q_layer_root_mean_squared_error: 0.9405 - y_t_layer_accuracy: 0.9872\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 89/130 [===================>..........] - ETA: 0s - loss: 1.0091 - y_q_layer_loss: 0.8986 - y_t_layer_loss: 0.1105 - y_q_layer_root_mean_squared_error: 0.9479 - y_t_layer_accuracy: 0.9860\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r120/130 [==========================>...] - ETA: 0s - loss: 0.9854 - y_q_layer_loss: 0.8835 - y_t_layer_loss: 0.1019 - y_q_layer_root_mean_squared_error: 0.9400 - y_t_layer_accuracy: 0.9880\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r130/130 [==============================] - 0s 2ms/step - loss: 0.9714 - y_q_layer_loss: 0.8705 - y_t_layer_loss: 0.1009 - y_q_layer_root_mean_squared_error: 0.9330 - y_t_layer_accuracy: 0.9874 - val_loss: 0.7957 - val_y_q_layer_loss: 0.7089 - val_y_t_layer_loss: 0.0868 - val_y_q_layer_root_mean_squared_error: 0.8420 - val_y_t_layer_accuracy: 0.9874\n",
      "Epoch 5/40\n",
      "\r  1/130 [..............................] - ETA: 0s - loss: 0.5201 - y_q_layer_loss: 0.4660 - y_t_layer_loss: 0.0541 - y_q_layer_root_mean_squared_error: 0.6826 - y_t_layer_accuracy: 1.0000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 32/130 [======>.......................] - ETA: 0s - loss: 0.8113 - y_q_layer_loss: 0.7259 - y_t_layer_loss: 0.0854 - y_q_layer_root_mean_squared_error: 0.8520 - y_t_layer_accuracy: 0.9873\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 62/130 [=============>................] - ETA: 0s - loss: 0.8118 - y_q_layer_loss: 0.7307 - y_t_layer_loss: 0.0812 - y_q_layer_root_mean_squared_error: 0.8548 - y_t_layer_accuracy: 0.9884\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 88/130 [===================>..........] - ETA: 0s - loss: 0.8138 - y_q_layer_loss: 0.7331 - y_t_layer_loss: 0.0807 - y_q_layer_root_mean_squared_error: 0.8562 - y_t_layer_accuracy: 0.9876\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r118/130 [==========================>...] - ETA: 0s - loss: 0.7820 - y_q_layer_loss: 0.7045 - y_t_layer_loss: 0.0775 - y_q_layer_root_mean_squared_error: 0.8393 - y_t_layer_accuracy: 0.9881\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r130/130 [==============================] - 0s 2ms/step - loss: 0.7787 - y_q_layer_loss: 0.7039 - y_t_layer_loss: 0.0748 - y_q_layer_root_mean_squared_error: 0.8390 - y_t_layer_accuracy: 0.9884 - val_loss: 0.6627 - val_y_q_layer_loss: 0.5964 - val_y_t_layer_loss: 0.0663 - val_y_q_layer_root_mean_squared_error: 0.7723 - val_y_t_layer_accuracy: 0.9894\n",
      "Epoch 6/40\n",
      "\r  1/130 [..............................] - ETA: 0s - loss: 0.8282 - y_q_layer_loss: 0.7555 - y_t_layer_loss: 0.0727 - y_q_layer_root_mean_squared_error: 0.8692 - y_t_layer_accuracy: 1.0000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 32/130 [======>.......................] - ETA: 0s - loss: 0.6790 - y_q_layer_loss: 0.6093 - y_t_layer_loss: 0.0697 - y_q_layer_root_mean_squared_error: 0.7805 - y_t_layer_accuracy: 0.9883\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 58/130 [============>.................] - ETA: 0s - loss: 0.6994 - y_q_layer_loss: 0.6311 - y_t_layer_loss: 0.0683 - y_q_layer_root_mean_squared_error: 0.7944 - y_t_layer_accuracy: 0.9881\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 87/130 [===================>..........] - ETA: 0s - loss: 0.6721 - y_q_layer_loss: 0.6113 - y_t_layer_loss: 0.0608 - y_q_layer_root_mean_squared_error: 0.7819 - y_t_layer_accuracy: 0.9896\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r117/130 [==========================>...] - ETA: 0s - loss: 0.6726 - y_q_layer_loss: 0.6109 - y_t_layer_loss: 0.0617 - y_q_layer_root_mean_squared_error: 0.7816 - y_t_layer_accuracy: 0.9893\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r130/130 [==============================] - 0s 2ms/step - loss: 0.6732 - y_q_layer_loss: 0.6122 - y_t_layer_loss: 0.0610 - y_q_layer_root_mean_squared_error: 0.7824 - y_t_layer_accuracy: 0.9894 - val_loss: 0.6108 - val_y_q_layer_loss: 0.5561 - val_y_t_layer_loss: 0.0547 - val_y_q_layer_root_mean_squared_error: 0.7457 - val_y_t_layer_accuracy: 0.9894\n",
      "Epoch 7/40\n",
      "\r  1/130 [..............................] - ETA: 0s - loss: 0.4160 - y_q_layer_loss: 0.3950 - y_t_layer_loss: 0.0209 - y_q_layer_root_mean_squared_error: 0.6285 - y_t_layer_accuracy: 1.0000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 32/130 [======>.......................] - ETA: 0s - loss: 0.5987 - y_q_layer_loss: 0.5448 - y_t_layer_loss: 0.0538 - y_q_layer_root_mean_squared_error: 0.7381 - y_t_layer_accuracy: 0.9922\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 63/130 [=============>................] - ETA: 0s - loss: 0.5886 - y_q_layer_loss: 0.5412 - y_t_layer_loss: 0.0474 - y_q_layer_root_mean_squared_error: 0.7356 - y_t_layer_accuracy: 0.9916\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 94/130 [====================>.........] - ETA: 0s - loss: 0.6085 - y_q_layer_loss: 0.5580 - y_t_layer_loss: 0.0504 - y_q_layer_root_mean_squared_error: 0.7470 - y_t_layer_accuracy: 0.9910\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r116/130 [=========================>....] - ETA: 0s - loss: 0.6220 - y_q_layer_loss: 0.5712 - y_t_layer_loss: 0.0509 - y_q_layer_root_mean_squared_error: 0.7557 - y_t_layer_accuracy: 0.9908\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r130/130 [==============================] - 0s 2ms/step - loss: 0.6211 - y_q_layer_loss: 0.5678 - y_t_layer_loss: 0.0533 - y_q_layer_root_mean_squared_error: 0.7535 - y_t_layer_accuracy: 0.9903 - val_loss: 0.5785 - val_y_q_layer_loss: 0.5301 - val_y_t_layer_loss: 0.0484 - val_y_q_layer_root_mean_squared_error: 0.7281 - val_y_t_layer_accuracy: 0.9894\n",
      "Epoch 8/40\n",
      "\r  1/130 [..............................] - ETA: 0s - loss: 0.4420 - y_q_layer_loss: 0.4239 - y_t_layer_loss: 0.0180 - y_q_layer_root_mean_squared_error: 0.6511 - y_t_layer_accuracy: 1.0000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 31/130 [======>.......................] - ETA: 0s - loss: 0.5652 - y_q_layer_loss: 0.5173 - y_t_layer_loss: 0.0479 - y_q_layer_root_mean_squared_error: 0.7192 - y_t_layer_accuracy: 0.9909\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 59/130 [============>.................] - ETA: 0s - loss: 0.5881 - y_q_layer_loss: 0.5326 - y_t_layer_loss: 0.0555 - y_q_layer_root_mean_squared_error: 0.7298 - y_t_layer_accuracy: 0.9883\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 85/130 [==================>...........] - ETA: 0s - loss: 0.5931 - y_q_layer_loss: 0.5389 - y_t_layer_loss: 0.0542 - y_q_layer_root_mean_squared_error: 0.7341 - y_t_layer_accuracy: 0.9897\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r109/130 [========================>.....] - ETA: 0s - loss: 0.5944 - y_q_layer_loss: 0.5460 - y_t_layer_loss: 0.0484 - y_q_layer_root_mean_squared_error: 0.7389 - y_t_layer_accuracy: 0.9911\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r130/130 [==============================] - 0s 2ms/step - loss: 0.5893 - y_q_layer_loss: 0.5404 - y_t_layer_loss: 0.0488 - y_q_layer_root_mean_squared_error: 0.7352 - y_t_layer_accuracy: 0.9911 - val_loss: 0.6038 - val_y_q_layer_loss: 0.5597 - val_y_t_layer_loss: 0.0442 - val_y_q_layer_root_mean_squared_error: 0.7481 - val_y_t_layer_accuracy: 0.9923\n",
      "Epoch 9/40\n",
      "\r  1/130 [..............................] - ETA: 0s - loss: 0.6787 - y_q_layer_loss: 0.6453 - y_t_layer_loss: 0.0333 - y_q_layer_root_mean_squared_error: 0.8033 - y_t_layer_accuracy: 1.0000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 23/130 [====>.........................] - ETA: 0s - loss: 0.5742 - y_q_layer_loss: 0.5333 - y_t_layer_loss: 0.0409 - y_q_layer_root_mean_squared_error: 0.7303 - y_t_layer_accuracy: 0.9932\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 52/130 [===========>..................] - ETA: 0s - loss: 0.5683 - y_q_layer_loss: 0.5112 - y_t_layer_loss: 0.0571 - y_q_layer_root_mean_squared_error: 0.7150 - y_t_layer_accuracy: 0.9898\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 82/130 [=================>............] - ETA: 0s - loss: 0.5499 - y_q_layer_loss: 0.4993 - y_t_layer_loss: 0.0506 - y_q_layer_root_mean_squared_error: 0.7066 - y_t_layer_accuracy: 0.9897\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r109/130 [========================>.....] - ETA: 0s - loss: 0.5613 - y_q_layer_loss: 0.5150 - y_t_layer_loss: 0.0463 - y_q_layer_root_mean_squared_error: 0.7176 - y_t_layer_accuracy: 0.9914\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r130/130 [==============================] - 1s 6ms/step - loss: 0.5687 - y_q_layer_loss: 0.5229 - y_t_layer_loss: 0.0457 - y_q_layer_root_mean_squared_error: 0.7231 - y_t_layer_accuracy: 0.9911 - val_loss: 0.5571 - val_y_q_layer_loss: 0.5172 - val_y_t_layer_loss: 0.0399 - val_y_q_layer_root_mean_squared_error: 0.7191 - val_y_t_layer_accuracy: 0.9923\n",
      "Epoch 10/40\n",
      "\r  1/130 [..............................] - ETA: 0s - loss: 0.7061 - y_q_layer_loss: 0.6629 - y_t_layer_loss: 0.0432 - y_q_layer_root_mean_squared_error: 0.8142 - y_t_layer_accuracy: 0.9688\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 32/130 [======>.......................] - ETA: 0s - loss: 0.5392 - y_q_layer_loss: 0.5012 - y_t_layer_loss: 0.0380 - y_q_layer_root_mean_squared_error: 0.7080 - y_t_layer_accuracy: 0.9941\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 63/130 [=============>................] - ETA: 0s - loss: 0.5245 - y_q_layer_loss: 0.4828 - y_t_layer_loss: 0.0417 - y_q_layer_root_mean_squared_error: 0.6949 - y_t_layer_accuracy: 0.9931\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 92/130 [====================>.........] - ETA: 0s - loss: 0.5402 - y_q_layer_loss: 0.4971 - y_t_layer_loss: 0.0431 - y_q_layer_root_mean_squared_error: 0.7051 - y_t_layer_accuracy: 0.9912\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r122/130 [===========================>..] - ETA: 0s - loss: 0.5540 - y_q_layer_loss: 0.5112 - y_t_layer_loss: 0.0429 - y_q_layer_root_mean_squared_error: 0.7149 - y_t_layer_accuracy: 0.9910\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r130/130 [==============================] - 0s 2ms/step - loss: 0.5488 - y_q_layer_loss: 0.5058 - y_t_layer_loss: 0.0430 - y_q_layer_root_mean_squared_error: 0.7112 - y_t_layer_accuracy: 0.9911 - val_loss: 0.5331 - val_y_q_layer_loss: 0.4955 - val_y_t_layer_loss: 0.0377 - val_y_q_layer_root_mean_squared_error: 0.7039 - val_y_t_layer_accuracy: 0.9923\n",
      "Epoch 11/40\n",
      "\r  1/130 [..............................] - ETA: 0s - loss: 0.4356 - y_q_layer_loss: 0.3898 - y_t_layer_loss: 0.0458 - y_q_layer_root_mean_squared_error: 0.6244 - y_t_layer_accuracy: 1.0000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 29/130 [=====>........................] - ETA: 0s - loss: 0.5021 - y_q_layer_loss: 0.4601 - y_t_layer_loss: 0.0420 - y_q_layer_root_mean_squared_error: 0.6783 - y_t_layer_accuracy: 0.9925\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 60/130 [============>.................] - ETA: 0s - loss: 0.5275 - y_q_layer_loss: 0.4904 - y_t_layer_loss: 0.0371 - y_q_layer_root_mean_squared_error: 0.7003 - y_t_layer_accuracy: 0.9922\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 91/130 [====================>.........] - ETA: 0s - loss: 0.5286 - y_q_layer_loss: 0.4899 - y_t_layer_loss: 0.0387 - y_q_layer_root_mean_squared_error: 0.6999 - y_t_layer_accuracy: 0.9918\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r120/130 [==========================>...] - ETA: 0s - loss: 0.5439 - y_q_layer_loss: 0.5055 - y_t_layer_loss: 0.0385 - y_q_layer_root_mean_squared_error: 0.7110 - y_t_layer_accuracy: 0.9922\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r130/130 [==============================] - 0s 2ms/step - loss: 0.5435 - y_q_layer_loss: 0.5023 - y_t_layer_loss: 0.0412 - y_q_layer_root_mean_squared_error: 0.7087 - y_t_layer_accuracy: 0.9915 - val_loss: 0.5936 - val_y_q_layer_loss: 0.5577 - val_y_t_layer_loss: 0.0358 - val_y_q_layer_root_mean_squared_error: 0.7468 - val_y_t_layer_accuracy: 0.9923\n",
      "Epoch 12/40\n",
      "\r  1/130 [..............................] - ETA: 0s - loss\n",
      "\n",
      "*** WARNING: max output size exceeded, skipping output. ***\n",
      "\n",
      "al_y_q_layer_loss: 0.4823 - val_y_t_layer_loss: 0.0244 - val_y_q_layer_root_mean_squared_error: 0.6945 - val_y_t_layer_accuracy: 0.9961\n",
      "Epoch 30/40\n",
      "\r  1/130 [..............................] - ETA: 0s - loss: 0.3769 - y_q_layer_loss: 0.3494 - y_t_layer_loss: 0.0275 - y_q_layer_root_mean_squared_error: 0.5911 - y_t_layer_accuracy: 1.0000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 32/130 [======>.......................] - ETA: 0s - loss: 0.4550 - y_q_layer_loss: 0.4100 - y_t_layer_loss: 0.0450 - y_q_layer_root_mean_squared_error: 0.6403 - y_t_layer_accuracy: 0.9922\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 60/130 [============>.................] - ETA: 0s - loss: 0.4545 - y_q_layer_loss: 0.4216 - y_t_layer_loss: 0.0330 - y_q_layer_root_mean_squared_error: 0.6493 - y_t_layer_accuracy: 0.9937\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 88/130 [===================>..........] - ETA: 0s - loss: 0.4570 - y_q_layer_loss: 0.4287 - y_t_layer_loss: 0.0283 - y_q_layer_root_mean_squared_error: 0.6548 - y_t_layer_accuracy: 0.9947\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r114/130 [=========================>....] - ETA: 0s - loss: 0.4517 - y_q_layer_loss: 0.4260 - y_t_layer_loss: 0.0257 - y_q_layer_root_mean_squared_error: 0.6527 - y_t_layer_accuracy: 0.9945\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r130/130 [==============================] - 0s 2ms/step - loss: 0.4533 - y_q_layer_loss: 0.4274 - y_t_layer_loss: 0.0259 - y_q_layer_root_mean_squared_error: 0.6538 - y_t_layer_accuracy: 0.9944 - val_loss: 0.5507 - val_y_q_layer_loss: 0.5264 - val_y_t_layer_loss: 0.0243 - val_y_q_layer_root_mean_squared_error: 0.7255 - val_y_t_layer_accuracy: 0.9952\n",
      "Epoch 31/40\n",
      "\r  1/130 [..............................] - ETA: 0s - loss: 0.2990 - y_q_layer_loss: 0.2968 - y_t_layer_loss: 0.0022 - y_q_layer_root_mean_squared_error: 0.5448 - y_t_layer_accuracy: 1.0000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 28/130 [=====>........................] - ETA: 0s - loss: 0.4659 - y_q_layer_loss: 0.4482 - y_t_layer_loss: 0.0178 - y_q_layer_root_mean_squared_error: 0.6695 - y_t_layer_accuracy: 0.9978\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 55/130 [===========>..................] - ETA: 0s - loss: 0.4800 - y_q_layer_loss: 0.4575 - y_t_layer_loss: 0.0225 - y_q_layer_root_mean_squared_error: 0.6764 - y_t_layer_accuracy: 0.9960\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 86/130 [==================>...........] - ETA: 0s - loss: 0.4778 - y_q_layer_loss: 0.4509 - y_t_layer_loss: 0.0269 - y_q_layer_root_mean_squared_error: 0.6715 - y_t_layer_accuracy: 0.9949\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r116/130 [=========================>....] - ETA: 0s - loss: 0.4565 - y_q_layer_loss: 0.4309 - y_t_layer_loss: 0.0256 - y_q_layer_root_mean_squared_error: 0.6564 - y_t_layer_accuracy: 0.9954\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r130/130 [==============================] - 0s 2ms/step - loss: 0.4548 - y_q_layer_loss: 0.4294 - y_t_layer_loss: 0.0253 - y_q_layer_root_mean_squared_error: 0.6553 - y_t_layer_accuracy: 0.9952 - val_loss: 0.5408 - val_y_q_layer_loss: 0.5154 - val_y_t_layer_loss: 0.0254 - val_y_q_layer_root_mean_squared_error: 0.7179 - val_y_t_layer_accuracy: 0.9932\n",
      "Epoch 32/40\n",
      "\r  1/130 [..............................] - ETA: 0s - loss: 0.3566 - y_q_layer_loss: 0.3518 - y_t_layer_loss: 0.0048 - y_q_layer_root_mean_squared_error: 0.5931 - y_t_layer_accuracy: 1.0000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 32/130 [======>.......................] - ETA: 0s - loss: 0.4553 - y_q_layer_loss: 0.4294 - y_t_layer_loss: 0.0259 - y_q_layer_root_mean_squared_error: 0.6553 - y_t_layer_accuracy: 0.9951\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 60/130 [============>.................] - ETA: 0s - loss: 0.4428 - y_q_layer_loss: 0.4179 - y_t_layer_loss: 0.0249 - y_q_layer_root_mean_squared_error: 0.6464 - y_t_layer_accuracy: 0.9953\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 90/130 [===================>..........] - ETA: 0s - loss: 0.4375 - y_q_layer_loss: 0.4148 - y_t_layer_loss: 0.0227 - y_q_layer_root_mean_squared_error: 0.6441 - y_t_layer_accuracy: 0.9958\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r120/130 [==========================>...] - ETA: 0s - loss: 0.4422 - y_q_layer_loss: 0.4179 - y_t_layer_loss: 0.0243 - y_q_layer_root_mean_squared_error: 0.6464 - y_t_layer_accuracy: 0.9951\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r130/130 [==============================] - 0s 2ms/step - loss: 0.4454 - y_q_layer_loss: 0.4204 - y_t_layer_loss: 0.0250 - y_q_layer_root_mean_squared_error: 0.6484 - y_t_layer_accuracy: 0.9952 - val_loss: 0.4910 - val_y_q_layer_loss: 0.4665 - val_y_t_layer_loss: 0.0244 - val_y_q_layer_root_mean_squared_error: 0.6830 - val_y_t_layer_accuracy: 0.9961\n",
      "Epoch 33/40\n",
      "\r  1/130 [..............................] - ETA: 0s - loss: 0.3718 - y_q_layer_loss: 0.3621 - y_t_layer_loss: 0.0097 - y_q_layer_root_mean_squared_error: 0.6017 - y_t_layer_accuracy: 1.0000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 32/130 [======>.......................] - ETA: 0s - loss: 0.4511 - y_q_layer_loss: 0.4116 - y_t_layer_loss: 0.0395 - y_q_layer_root_mean_squared_error: 0.6416 - y_t_layer_accuracy: 0.9922\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 62/130 [=============>................] - ETA: 0s - loss: 0.4494 - y_q_layer_loss: 0.4147 - y_t_layer_loss: 0.0347 - y_q_layer_root_mean_squared_error: 0.6439 - y_t_layer_accuracy: 0.9929\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 90/130 [===================>..........] - ETA: 0s - loss: 0.4561 - y_q_layer_loss: 0.4269 - y_t_layer_loss: 0.0293 - y_q_layer_root_mean_squared_error: 0.6533 - y_t_layer_accuracy: 0.9937\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r120/130 [==========================>...] - ETA: 0s - loss: 0.4542 - y_q_layer_loss: 0.4283 - y_t_layer_loss: 0.0259 - y_q_layer_root_mean_squared_error: 0.6545 - y_t_layer_accuracy: 0.9948\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r130/130 [==============================] - 0s 2ms/step - loss: 0.4566 - y_q_layer_loss: 0.4319 - y_t_layer_loss: 0.0247 - y_q_layer_root_mean_squared_error: 0.6572 - y_t_layer_accuracy: 0.9952 - val_loss: 0.5126 - val_y_q_layer_loss: 0.4885 - val_y_t_layer_loss: 0.0241 - val_y_q_layer_root_mean_squared_error: 0.6989 - val_y_t_layer_accuracy: 0.9961\n",
      "Epoch 34/40\n",
      "\r  1/130 [..............................] - ETA: 0s - loss: 0.6358 - y_q_layer_loss: 0.3709 - y_t_layer_loss: 0.2649 - y_q_layer_root_mean_squared_error: 0.6090 - y_t_layer_accuracy: 0.9688\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 32/130 [======>.......................] - ETA: 0s - loss: 0.4333 - y_q_layer_loss: 0.4095 - y_t_layer_loss: 0.0239 - y_q_layer_root_mean_squared_error: 0.6399 - y_t_layer_accuracy: 0.9971\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 63/130 [=============>................] - ETA: 0s - loss: 0.4327 - y_q_layer_loss: 0.4135 - y_t_layer_loss: 0.0192 - y_q_layer_root_mean_squared_error: 0.6430 - y_t_layer_accuracy: 0.9970\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 93/130 [====================>.........] - ETA: 0s - loss: 0.4557 - y_q_layer_loss: 0.4298 - y_t_layer_loss: 0.0259 - y_q_layer_root_mean_squared_error: 0.6556 - y_t_layer_accuracy: 0.9950\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r122/130 [===========================>..] - ETA: 0s - loss: 0.4438 - y_q_layer_loss: 0.4200 - y_t_layer_loss: 0.0239 - y_q_layer_root_mean_squared_error: 0.6480 - y_t_layer_accuracy: 0.9954\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r130/130 [==============================] - 0s 2ms/step - loss: 0.4425 - y_q_layer_loss: 0.4184 - y_t_layer_loss: 0.0241 - y_q_layer_root_mean_squared_error: 0.6468 - y_t_layer_accuracy: 0.9952 - val_loss: 0.5075 - val_y_q_layer_loss: 0.4834 - val_y_t_layer_loss: 0.0241 - val_y_q_layer_root_mean_squared_error: 0.6953 - val_y_t_layer_accuracy: 0.9961\n",
      "Epoch 35/40\n",
      "\r  1/130 [..............................] - ETA: 0s - loss: 0.3910 - y_q_layer_loss: 0.3883 - y_t_layer_loss: 0.0026 - y_q_layer_root_mean_squared_error: 0.6232 - y_t_layer_accuracy: 1.0000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 32/130 [======>.......................] - ETA: 0s - loss: 0.4465 - y_q_layer_loss: 0.4096 - y_t_layer_loss: 0.0369 - y_q_layer_root_mean_squared_error: 0.6400 - y_t_layer_accuracy: 0.9912\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 63/130 [=============>................] - ETA: 0s - loss: 0.4498 - y_q_layer_loss: 0.4203 - y_t_layer_loss: 0.0295 - y_q_layer_root_mean_squared_error: 0.6483 - y_t_layer_accuracy: 0.9926\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 94/130 [====================>.........] - ETA: 0s - loss: 0.4552 - y_q_layer_loss: 0.4256 - y_t_layer_loss: 0.0296 - y_q_layer_root_mean_squared_error: 0.6524 - y_t_layer_accuracy: 0.9934\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r124/130 [===========================>..] - ETA: 0s - loss: 0.4363 - y_q_layer_loss: 0.4116 - y_t_layer_loss: 0.0247 - y_q_layer_root_mean_squared_error: 0.6415 - y_t_layer_accuracy: 0.9945\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r130/130 [==============================] - 0s 2ms/step - loss: 0.4384 - y_q_layer_loss: 0.4144 - y_t_layer_loss: 0.0240 - y_q_layer_root_mean_squared_error: 0.6437 - y_t_layer_accuracy: 0.9947 - val_loss: 0.5197 - val_y_q_layer_loss: 0.4956 - val_y_t_layer_loss: 0.0241 - val_y_q_layer_root_mean_squared_error: 0.7040 - val_y_t_layer_accuracy: 0.9952\n",
      "Epoch 36/40\n",
      "\r  1/130 [..............................] - ETA: 0s - loss: 1.0490 - y_q_layer_loss: 1.0448 - y_t_layer_loss: 0.0041 - y_q_layer_root_mean_squared_error: 1.0222 - y_t_layer_accuracy: 1.0000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 30/130 [=====>........................] - ETA: 0s - loss: 0.4815 - y_q_layer_loss: 0.4575 - y_t_layer_loss: 0.0239 - y_q_layer_root_mean_squared_error: 0.6764 - y_t_layer_accuracy: 0.9937\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 57/130 [============>.................] - ETA: 0s - loss: 0.4562 - y_q_layer_loss: 0.4309 - y_t_layer_loss: 0.0253 - y_q_layer_root_mean_squared_error: 0.6564 - y_t_layer_accuracy: 0.9951\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 87/130 [===================>..........] - ETA: 0s - loss: 0.4480 - y_q_layer_loss: 0.4247 - y_t_layer_loss: 0.0233 - y_q_layer_root_mean_squared_error: 0.6517 - y_t_layer_accuracy: 0.9957\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r117/130 [==========================>...] - ETA: 0s - loss: 0.4399 - y_q_layer_loss: 0.4200 - y_t_layer_loss: 0.0198 - y_q_layer_root_mean_squared_error: 0.6481 - y_t_layer_accuracy: 0.9963\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r130/130 [==============================] - 0s 2ms/step - loss: 0.4413 - y_q_layer_loss: 0.4176 - y_t_layer_loss: 0.0237 - y_q_layer_root_mean_squared_error: 0.6462 - y_t_layer_accuracy: 0.9952 - val_loss: 0.5206 - val_y_q_layer_loss: 0.4968 - val_y_t_layer_loss: 0.0238 - val_y_q_layer_root_mean_squared_error: 0.7048 - val_y_t_layer_accuracy: 0.9952\n",
      "Epoch 37/40\n",
      "\r  1/130 [..............................] - ETA: 0s - loss: 0.3779 - y_q_layer_loss: 0.3686 - y_t_layer_loss: 0.0094 - y_q_layer_root_mean_squared_error: 0.6071 - y_t_layer_accuracy: 1.0000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 31/130 [======>.......................] - ETA: 0s - loss: 0.4252 - y_q_layer_loss: 0.4121 - y_t_layer_loss: 0.0131 - y_q_layer_root_mean_squared_error: 0.6419 - y_t_layer_accuracy: 0.9990\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 63/130 [=============>................] - ETA: 0s - loss: 0.4171 - y_q_layer_loss: 0.3937 - y_t_layer_loss: 0.0234 - y_q_layer_root_mean_squared_error: 0.6275 - y_t_layer_accuracy: 0.9965\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 94/130 [====================>.........] - ETA: 0s - loss: 0.4270 - y_q_layer_loss: 0.4051 - y_t_layer_loss: 0.0219 - y_q_layer_root_mean_squared_error: 0.6364 - y_t_layer_accuracy: 0.9963\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r125/130 [===========================>..] - ETA: 0s - loss: 0.4350 - y_q_layer_loss: 0.4131 - y_t_layer_loss: 0.0219 - y_q_layer_root_mean_squared_error: 0.6428 - y_t_layer_accuracy: 0.9955\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r130/130 [==============================] - 0s 2ms/step - loss: 0.4363 - y_q_layer_loss: 0.4130 - y_t_layer_loss: 0.0233 - y_q_layer_root_mean_squared_error: 0.6426 - y_t_layer_accuracy: 0.9952 - val_loss: 0.5212 - val_y_q_layer_loss: 0.4970 - val_y_t_layer_loss: 0.0241 - val_y_q_layer_root_mean_squared_error: 0.7050 - val_y_t_layer_accuracy: 0.9961\n",
      "Epoch 38/40\n",
      "\r  1/130 [..............................] - ETA: 0s - loss: 0.3384 - y_q_layer_loss: 0.3357 - y_t_layer_loss: 0.0027 - y_q_layer_root_mean_squared_error: 0.5794 - y_t_layer_accuracy: 1.0000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 26/130 [=====>........................] - ETA: 0s - loss: 0.3850 - y_q_layer_loss: 0.3656 - y_t_layer_loss: 0.0194 - y_q_layer_root_mean_squared_error: 0.6046 - y_t_layer_accuracy: 0.9976\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 58/130 [============>.................] - ETA: 0s - loss: 0.4256 - y_q_layer_loss: 0.4010 - y_t_layer_loss: 0.0246 - y_q_layer_root_mean_squared_error: 0.6332 - y_t_layer_accuracy: 0.9952\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 86/130 [==================>...........] - ETA: 0s - loss: 0.4343 - y_q_layer_loss: 0.4131 - y_t_layer_loss: 0.0211 - y_q_layer_root_mean_squared_error: 0.6428 - y_t_layer_accuracy: 0.9960\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r117/130 [==========================>...] - ETA: 0s - loss: 0.4283 - y_q_layer_loss: 0.4078 - y_t_layer_loss: 0.0205 - y_q_layer_root_mean_squared_error: 0.6386 - y_t_layer_accuracy: 0.9960\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r130/130 [==============================] - 0s 2ms/step - loss: 0.4303 - y_q_layer_loss: 0.4073 - y_t_layer_loss: 0.0231 - y_q_layer_root_mean_squared_error: 0.6382 - y_t_layer_accuracy: 0.9954 - val_loss: 0.5248 - val_y_q_layer_loss: 0.4999 - val_y_t_layer_loss: 0.0248 - val_y_q_layer_root_mean_squared_error: 0.7071 - val_y_t_layer_accuracy: 0.9952\n",
      "Epoch 39/40\n",
      "\r  1/130 [..............................] - ETA: 0s - loss: 0.4376 - y_q_layer_loss: 0.4307 - y_t_layer_loss: 0.0069 - y_q_layer_root_mean_squared_error: 0.6563 - y_t_layer_accuracy: 1.0000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 31/130 [======>.......................] - ETA: 0s - loss: 0.4275 - y_q_layer_loss: 0.4079 - y_t_layer_loss: 0.0197 - y_q_layer_root_mean_squared_error: 0.6387 - y_t_layer_accuracy: 0.9970\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 61/130 [=============>................] - ETA: 0s - loss: 0.4198 - y_q_layer_loss: 0.3980 - y_t_layer_loss: 0.0218 - y_q_layer_root_mean_squared_error: 0.6309 - y_t_layer_accuracy: 0.9959\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 91/130 [====================>.........] - ETA: 0s - loss: 0.4160 - y_q_layer_loss: 0.3915 - y_t_layer_loss: 0.0245 - y_q_layer_root_mean_squared_error: 0.6257 - y_t_layer_accuracy: 0.9952\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r122/130 [===========================>..] - ETA: 0s - loss: 0.4365 - y_q_layer_loss: 0.4143 - y_t_layer_loss: 0.0222 - y_q_layer_root_mean_squared_error: 0.6436 - y_t_layer_accuracy: 0.9959\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r130/130 [==============================] - 1s 6ms/step - loss: 0.4420 - y_q_layer_loss: 0.4192 - y_t_layer_loss: 0.0228 - y_q_layer_root_mean_squared_error: 0.6475 - y_t_layer_accuracy: 0.9956 - val_loss: 0.6154 - val_y_q_layer_loss: 0.5918 - val_y_t_layer_loss: 0.0235 - val_y_q_layer_root_mean_squared_error: 0.7693 - val_y_t_layer_accuracy: 0.9961\n",
      "Epoch 40/40\n",
      "\r  1/130 [..............................] - ETA: 0s - loss: 0.5772 - y_q_layer_loss: 0.5680 - y_t_layer_loss: 0.0092 - y_q_layer_root_mean_squared_error: 0.7537 - y_t_layer_accuracy: 1.0000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 32/130 [======>.......................] - ETA: 0s - loss: 0.4059 - y_q_layer_loss: 0.3818 - y_t_layer_loss: 0.0241 - y_q_layer_root_mean_squared_error: 0.6179 - y_t_layer_accuracy: 0.9941\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 63/130 [=============>................] - ETA: 0s - loss: 0.4245 - y_q_layer_loss: 0.3938 - y_t_layer_loss: 0.0307 - y_q_layer_root_mean_squared_error: 0.6275 - y_t_layer_accuracy: 0.9931\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 91/130 [====================>.........] - ETA: 0s - loss: 0.4394 - y_q_layer_loss: 0.4150 - y_t_layer_loss: 0.0244 - y_q_layer_root_mean_squared_error: 0.6442 - y_t_layer_accuracy: 0.9945\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r117/130 [==========================>...] - ETA: 0s - loss: 0.4383 - y_q_layer_loss: 0.4139 - y_t_layer_loss: 0.0244 - y_q_layer_root_mean_squared_error: 0.6433 - y_t_layer_accuracy: 0.9949\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r130/130 [==============================] - 0s 2ms/step - loss: 0.4396 - y_q_layer_loss: 0.4169 - y_t_layer_loss: 0.0227 - y_q_layer_root_mean_squared_error: 0.6457 - y_t_layer_accuracy: 0.9954 - val_loss: 0.5163 - val_y_q_layer_loss: 0.4927 - val_y_t_layer_loss: 0.0235 - val_y_q_layer_root_mean_squared_error: 0.7019 - val_y_t_layer_accuracy: 0.9961\n",
      "INFO:tensorflow:Assets written to: /local_disk0/repl_tmp_data/ReplId-794e4-84dac-c0983-9/tmpmh5lnxok/model/data/model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /local_disk0/repl_tmp_data/ReplId-794e4-84dac-c0983-9/tmpmh5lnxok/model/data/model/assets\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c76ff9c05b249f8b032a19b9be00002",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f97110cb47e64989b16bb7e5916b726b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "log_dir=\"runs/our_experiment\"\n",
    "\n",
    "history = model.fit(train_X, train_y, \n",
    "                    epochs = 40, validation_data=(val_X, val_y), callbacks=[TensorBoard(log_dir=log_dir)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9db42c60-3861-452a-868a-3d7802da22b9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r 1/33 [..............................] - ETA: 1s - loss: 0.6386 - y_q_layer_loss: 0.4723 - y_t_layer_loss: 0.1663 - y_q_layer_root_mean_squared_error: 0.6873 - y_t_layer_accuracy: 0.9688\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r33/33 [==============================] - 0s 1ms/step - loss: 0.5163 - y_q_layer_loss: 0.4927 - y_t_layer_loss: 0.0235 - y_q_layer_root_mean_squared_error: 0.7019 - y_t_layer_accuracy: 0.9961\n",
      "\n",
      "loss: 0.5162505507469177\n",
      "wine_quality_loss: 0.4927070736885071\n",
      "wine_type_loss: 0.023543452844023705\n",
      "wine_quality_rmse: 0.7019309401512146\n",
      "wine_type_accuracy: 0.9961315393447876\n"
     ]
    }
   ],
   "source": [
    "loss, wine_quality_loss, wine_type_loss, wine_quality_rmse, wine_type_accuracy = model.evaluate(x=val_X, y=val_y)\n",
    "\n",
    "print()\n",
    "print(f'loss: {loss}')\n",
    "print(f'wine_quality_loss: {wine_quality_loss}')\n",
    "print(f'wine_type_loss: {wine_type_loss}')\n",
    "print(f'wine_quality_rmse: {wine_quality_rmse}')\n",
    "print(f'wine_type_accuracy: {wine_type_accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e59210d-5efb-41a8-989d-3f821178b277",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "La cella di codice successiva carica l'estensione di TensorBoard per Jupyter Notebook utilizzando il comando `%load_ext tensorboard`. TensorBoard è uno strumento di visualizzazione fornito da TensorFlow che consente di monitorare e analizzare i modelli di machine learning. L'estensione di TensorBoard consente di visualizzare i log di TensorBoard direttamente all'interno di Jupyter Notebook.\n",
    "\n",
    "L'importazione dell'estensione di TensorBoard è utile quando si desidera utilizzare TensorBoard per visualizzare i log di addestramento e monitorare le prestazioni del modello durante l'addestramento. L'estensione di TensorBoard semplifica l'uso di TensorBoard all'interno di Jupyter Notebook, consentendo di visualizzare i log senza dover aprire un'altra finestra o terminale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11c4de3d-f5a4-49bd-abbb-c71fb8d66b14",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-10 10:02:01.816570: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Import tensorboard logger from PyTorch\n",
    "#from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Load tensorboard extension for Jupyter Notebook, only need to start TB in the notebook\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f4cda58-90c1-492f-9ca2-948dc39a38fc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your log directory might be ephemeral to the cluster, which will be deleted after cluster termination or restart. You can choose a log directory under /dbfs/ or /Volumes/ to persist your logs in DBFS or UC Volumes.\n",
      "Tensorboard may not be displayed in the notebook cell output when 'Third-party iFraming prevention' is disabled. You can still use Tensorboard by clicking the link below to open Tensorboard in a new tab. To enable Tensorboard in notebook cell output, please ask your workspace admin to enable 'Third-party iFraming prevention'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Launching TensorBoard..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div style=\"margin-bottom: 16px\">\n",
       "            <a href=\"/driver-proxy/o/1936086074818047/0216-140044-c8r7yflb/6007/\">\n",
       "                Open in a new tab\n",
       "            </a>\n",
       "            <span style=\"margin-left: 1em; color: #a3a3a3\">Note: TensorBoard is only available when this notebook remains attached to the cluster.</span>\n",
       "        </div>\n",
       "        <div style=\"margin-bottom: 16px\">\n",
       "            <span style=\"color: #a3a3a3\">Note: This cell needs to be re-run for TensorBoard to be available if this notebook is imported into a different workspace.</span>\n",
       "        </div>\n",
       "        <iframe id=\"%tensorboard-frame-ddec5eccd030ca5\" width=\"100%\" height=\"800\" frameborder=\"0\" src=\"/driver-proxy/o/1936086074818047/0216-140044-c8r7yflb/6007/\"></iframe>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir runs/our_experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "969c3919-f869-4067-87d5-6ce4a176f11a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Deep Learning Multi input Multi output - Wine Quality",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
