{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Version: \"3.10.5\"\n",
    "# Java Version: \"1.8.0_421\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openjdk version \"1.8.0_422-422\"\n",
      "OpenJDK Runtime Environment (build 1.8.0_422-422-b05)\n",
      "OpenJDK 64-Bit Server VM (build 25.422-b05, mixed mode)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "try:\n",
    "    # Esegue il comando per ottenere la versione di Java\n",
    "    version = subprocess.check_output(['java', '-version'], stderr=subprocess.STDOUT)\n",
    "    print(version.decode('utf-8'))\n",
    "except FileNotFoundError:\n",
    "    # Gestisce il caso in cui Java non sia installato\n",
    "    print(\"Java non è installato sul sistema. Si prega di installare Java 8 o successiva\")\n",
    "except subprocess.CalledProcessError as e:\n",
    "    # Gestisce errori legati all'esecuzione del comando\n",
    "    print(f\"Errore durante l'esecuzione del comando Java: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\OpenLogic\\jdk-8.0.422.05-hotspot\n",
      "C:\\hadoop-3.3.6\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(os.getenv(\"JAVA_HOME\"))\n",
    "print(os.getenv(\"HADOOP_HOME\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark==3.4.1 in c:\\users\\pagan133\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from -r ../../requirements.txt (line 1)) (3.4.1)\n",
      "Requirement already satisfied: spark-nlp==5.4.1 in c:\\users\\pagan133\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from -r ../../requirements.txt (line 2)) (5.4.1)\n",
      "Requirement already satisfied: numpy==1.26.4 in c:\\users\\pagan133\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from -r ../../requirements.txt (line 3)) (1.26.4)\n",
      "Requirement already satisfied: pandas==2.2.2 in c:\\users\\pagan133\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from -r ../../requirements.txt (line 4)) (2.2.2)\n",
      "Requirement already satisfied: plotly==5.22.0 in c:\\users\\pagan133\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from -r ../../requirements.txt (line 5)) (5.22.0)\n",
      "Requirement already satisfied: scikit-learn==1.4.2 in c:\\users\\pagan133\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from -r ../../requirements.txt (line 6)) (1.4.2)\n",
      "Requirement already satisfied: tqdm==4.66.4 in c:\\users\\pagan133\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from -r ../../requirements.txt (line 7)) (4.66.4)\n",
      "Requirement already satisfied: seaborn==0.13.2 in c:\\users\\pagan133\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from -r ../../requirements.txt (line 8)) (0.13.2)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in c:\\users\\pagan133\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pyspark==3.4.1->-r ../../requirements.txt (line 1)) (0.10.9.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\pagan133\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas==2.2.2->-r ../../requirements.txt (line 4)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\pagan133\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas==2.2.2->-r ../../requirements.txt (line 4)) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\pagan133\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas==2.2.2->-r ../../requirements.txt (line 4)) (2024.1)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\pagan133\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from plotly==5.22.0->-r ../../requirements.txt (line 5)) (8.0.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\pagan133\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from plotly==5.22.0->-r ../../requirements.txt (line 5)) (21.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\pagan133\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn==1.4.2->-r ../../requirements.txt (line 6)) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\pagan133\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn==1.4.2->-r ../../requirements.txt (line 6)) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\pagan133\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn==1.4.2->-r ../../requirements.txt (line 6)) (3.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\pagan133\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm==4.66.4->-r ../../requirements.txt (line 7)) (0.4.5)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in c:\\users\\pagan133\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from seaborn==0.13.2->-r ../../requirements.txt (line 8)) (3.5.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\pagan133\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn==0.13.2->-r ../../requirements.txt (line 8)) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\pagan133\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn==0.13.2->-r ../../requirements.txt (line 8)) (4.37.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\pagan133\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn==0.13.2->-r ../../requirements.txt (line 8)) (1.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\pagan133\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn==0.13.2->-r ../../requirements.txt (line 8)) (9.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\pagan133\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn==0.13.2->-r ../../requirements.txt (line 8)) (3.0.9)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\pagan133\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.8.2->pandas==2.2.2->-r ../../requirements.txt (line 4)) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#Commentare se si importa il file in env anaconda\n",
    "%pip install -r ../../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sparknlp\n",
    "\n",
    "# create or get Spark Session\n",
    "#spark = sparknlp.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark NLP\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"16G\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"2000M\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"0\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:5.4.1\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark NLP version: 5.4.1\n",
      "Apache Spark version: 3.4.1\n"
     ]
    }
   ],
   "source": [
    "# Verifica della versione di Spark NLP\n",
    "print(\"Spark NLP version:\", sparknlp.version())\n",
    "print(\"Apache Spark version:\", spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pagan133\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\computation\\expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Aggiungi il percorso del modulo al PYTHONPATH\n",
    "sys.path.append(os.path.abspath('../../Common'))\n",
    "\n",
    "import UtilityNLP as nlpUtils\n",
    "import UtilityClustering as cltUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "985808a6-f148-4eaf-a442-2e0d5a913235",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|         descrizione|\n",
      "+--------------------+\n",
      "|ACCETTAZIONE DELL...|\n",
      "|CONDIZIONI GENERA...|\n",
      "| INFORMATIVA PRIVACY|\n",
      "|ACCETTAZIONE DELL...|\n",
      "|ACCETTAZIONE DELL...|\n",
      "|ACCETTAZIONE DELL...|\n",
      "|ACCETTAZIONE DELL...|\n",
      "| INFORMATIVA PRIVACY|\n",
      "| INFORMATIVA PRIVACY|\n",
      "| INFORMATIVA PRIVACY|\n",
      "| INFORMATIVA PRIVACY|\n",
      "|      SCHEDA CLIENTE|\n",
      "|      SCHEDA CLIENTE|\n",
      "|ALLEGATO AMMINIST...|\n",
      "|ALLEGATO AMMINIST...|\n",
      "|ALLEGATO AMMINIST...|\n",
      "|manca la sede leg...|\n",
      "|      SCHEDA CLIENTE|\n",
      "|      SCHEDA CLIENTE|\n",
      "|      SCHEDA CLIENTE|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Lettura Dataframe\n",
    "df=spark.read.option(\"header\",True).csv('../dati/input/esempio_frasi_1.csv')\n",
    "df.cache()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3352a7dc-38b1-4b6a-975d-24bd5929cee9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence_detector_dl download started this may take some time.\n",
      "Approximate size to download 514,9 KB\n",
      "[ | ]\n",
      "An error occurred while calling z:com.johnsnowlabs.nlp.pretrained.PythonResourceDownloader.downloadModel.\n",
      ": java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z\n",
      "\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\n",
      "\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)\n",
      "\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1249)\n",
      "\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1454)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\n",
      "\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\n",
      "\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\n",
      "\tat org.apache.hadoop.fs.FileSystem$4.<init>(FileSystem.java:2180)\n",
      "\tat org.apache.hadoop.fs.FileSystem.listLocatedStatus(FileSystem.java:2179)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.listLocatedStatus(ChecksumFileSystem.java:783)\n",
      "\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:285)\n",
      "\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\n",
      "\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\n",
      "\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:208)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:291)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:287)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:291)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:287)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$take$1(RDD.scala:1441)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n",
      "\tat org.apache.spark.rdd.RDD.take(RDD.scala:1435)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$first$1(RDD.scala:1476)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n",
      "\tat org.apache.spark.rdd.RDD.first(RDD.scala:1476)\n",
      "\tat org.apache.spark.ml.util.DefaultParamsReader$.loadMetadata(ReadWrite.scala:587)\n",
      "\tat org.apache.spark.ml.util.DefaultParamsReader.load(ReadWrite.scala:465)\n",
      "\tat com.johnsnowlabs.nlp.FeaturesReader.load(ParamsAndFeaturesReadable.scala:31)\n",
      "\tat com.johnsnowlabs.nlp.FeaturesReader.load(ParamsAndFeaturesReadable.scala:24)\n",
      "\tat com.johnsnowlabs.nlp.pretrained.ResourceDownloader$.downloadModel(ResourceDownloader.scala:515)\n",
      "\tat com.johnsnowlabs.nlp.pretrained.ResourceDownloader$.downloadModel(ResourceDownloader.scala:507)\n",
      "\tat com.johnsnowlabs.nlp.pretrained.PythonResourceDownloader$.downloadModel(ResourceDownloader.scala:714)\n",
      "\tat com.johnsnowlabs.nlp.pretrained.PythonResourceDownloader.downloadModel(ResourceDownloader.scala)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "[OK!]\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:com.johnsnowlabs.nlp.pretrained.PythonResourceDownloader.downloadModel.\n: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1249)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1454)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.fs.FileSystem$4.<init>(FileSystem.java:2180)\r\n\tat org.apache.hadoop.fs.FileSystem.listLocatedStatus(FileSystem.java:2179)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listLocatedStatus(ChecksumFileSystem.java:783)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:285)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\r\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:208)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:291)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:287)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:291)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:287)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$take$1(RDD.scala:1441)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\r\n\tat org.apache.spark.rdd.RDD.take(RDD.scala:1435)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$first$1(RDD.scala:1476)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\r\n\tat org.apache.spark.rdd.RDD.first(RDD.scala:1476)\r\n\tat org.apache.spark.ml.util.DefaultParamsReader$.loadMetadata(ReadWrite.scala:587)\r\n\tat org.apache.spark.ml.util.DefaultParamsReader.load(ReadWrite.scala:465)\r\n\tat com.johnsnowlabs.nlp.FeaturesReader.load(ParamsAndFeaturesReadable.scala:31)\r\n\tat com.johnsnowlabs.nlp.FeaturesReader.load(ParamsAndFeaturesReadable.scala:24)\r\n\tat com.johnsnowlabs.nlp.pretrained.ResourceDownloader$.downloadModel(ResourceDownloader.scala:515)\r\n\tat com.johnsnowlabs.nlp.pretrained.ResourceDownloader$.downloadModel(ResourceDownloader.scala:507)\r\n\tat com.johnsnowlabs.nlp.pretrained.PythonResourceDownloader$.downloadModel(ResourceDownloader.scala:714)\r\n\tat com.johnsnowlabs.nlp.pretrained.PythonResourceDownloader.downloadModel(ResourceDownloader.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Definizione della pipeline per il Sentence Embedding con BERT\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mnlpUtils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnlp_pipeline_bert_sentence_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdescrizione\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mfit(df)\n\u001b[0;32m      3\u001b[0m result_bert \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtransform(df)\n",
      "File \u001b[1;32mc:\\Users\\pagan133\\OneDrive - Poste Italiane S.p.A\\Documenti\\GitHub\\ML_Project\\SemanticInterpreter\\Common\\UtilityNLP.py:50\u001b[0m, in \u001b[0;36mnlp_pipeline_bert_sentence_embedding\u001b[1;34m(text_col)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Pipeline\n\u001b[0;32m     45\u001b[0m documentAssembler \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     46\u001b[0m     DocumentAssembler()\u001b[38;5;241m.\u001b[39msetInputCol(text_col)\u001b[38;5;241m.\u001b[39msetOutputCol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocument\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     47\u001b[0m )\n\u001b[0;32m     49\u001b[0m sentencerDL \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m---> 50\u001b[0m     \u001b[43mSentenceDetectorDLModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msentence_detector_dl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mxx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;241m.\u001b[39msetInputCols([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocument\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;241m.\u001b[39msetOutputCol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentences\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     53\u001b[0m )\n\u001b[0;32m     55\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m Tokenizer()\u001b[38;5;241m.\u001b[39msetInputCols(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentences\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msetOutputCol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     57\u001b[0m normalizer \u001b[38;5;241m=\u001b[39m Normalizer()\u001b[38;5;241m.\u001b[39msetInputCols([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39msetOutputCol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnormalized\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msetCleanupPatterns([\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m[^\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms]|http\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mS+|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md+|[A-Z][a-z]+\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m])\u001b[38;5;241m.\u001b[39msetLowercase(\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\pagan133\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sparknlp\\annotator\\sentence\\sentence_detector_dl.py:467\u001b[0m, in \u001b[0;36mSentenceDetectorDLModel.pretrained\u001b[1;34m(name, lang, remote_loc)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;124;03m\"\"\"Downloads and loads a pretrained model.\u001b[39;00m\n\u001b[0;32m    450\u001b[0m \n\u001b[0;32m    451\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    464\u001b[0m \u001b[38;5;124;03m    The restored model\u001b[39;00m\n\u001b[0;32m    465\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    466\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msparknlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpretrained\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ResourceDownloader\n\u001b[1;32m--> 467\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mResourceDownloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownloadModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSentenceDetectorDLModel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlang\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremote_loc\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pagan133\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sparknlp\\pretrained\\resource_downloader.py:98\u001b[0m, in \u001b[0;36mResourceDownloader.downloadModel\u001b[1;34m(reader, name, language, remote_loc, j_dwn)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     97\u001b[0m     sys\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e))\n\u001b[1;32m---> 98\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    100\u001b[0m     stop_threads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\pagan133\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sparknlp\\pretrained\\resource_downloader.py:95\u001b[0m, in \u001b[0;36mResourceDownloader.downloadModel\u001b[1;34m(reader, name, language, remote_loc, j_dwn)\u001b[0m\n\u001b[0;32m     93\u001b[0m t1\u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 95\u001b[0m     j_obj \u001b[38;5;241m=\u001b[39m \u001b[43m_internal\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_DownloadModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremote_loc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj_dwn\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mapply()\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     97\u001b[0m     sys\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e))\n",
      "File \u001b[1;32mc:\\Users\\pagan133\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sparknlp\\internal\\__init__.py:537\u001b[0m, in \u001b[0;36m_DownloadModel.__init__\u001b[1;34m(self, reader, name, language, remote_loc, validator)\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, reader, name, language, remote_loc, validator):\n\u001b[1;32m--> 537\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_DownloadModel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    538\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcom.johnsnowlabs.nlp.pretrained.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mvalidator\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.downloadModel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    539\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    540\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    541\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    542\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremote_loc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    543\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pagan133\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sparknlp\\internal\\extended_java_wrapper.py:27\u001b[0m, in \u001b[0;36mExtendedJavaWrapper.__init__\u001b[1;34m(self, java_obj, *args)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28msuper\u001b[39m(ExtendedJavaWrapper, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(java_obj)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msc \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnew_java_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjava_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj\n",
      "File \u001b[1;32mc:\\Users\\pagan133\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sparknlp\\internal\\extended_java_wrapper.py:37\u001b[0m, in \u001b[0;36mExtendedJavaWrapper.new_java_obj\u001b[1;34m(self, java_class, *args)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnew_java_obj\u001b[39m(\u001b[38;5;28mself\u001b[39m, java_class, \u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m---> 37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_java_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjava_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pagan133\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\ml\\wrapper.py:86\u001b[0m, in \u001b[0;36mJavaWrapper._new_java_obj\u001b[1;34m(java_class, *args)\u001b[0m\n\u001b[0;32m     84\u001b[0m     java_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(java_obj, name)\n\u001b[0;32m     85\u001b[0m java_args \u001b[38;5;241m=\u001b[39m [_py2java(sc, arg) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args]\n\u001b[1;32m---> 86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjava_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mjava_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pagan133\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\pagan133\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    171\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\pagan133\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:com.johnsnowlabs.nlp.pretrained.PythonResourceDownloader.downloadModel.\n: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1249)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1454)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.fs.FileSystem$4.<init>(FileSystem.java:2180)\r\n\tat org.apache.hadoop.fs.FileSystem.listLocatedStatus(FileSystem.java:2179)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listLocatedStatus(ChecksumFileSystem.java:783)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:285)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\r\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:208)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:291)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:287)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:291)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:287)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$take$1(RDD.scala:1441)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\r\n\tat org.apache.spark.rdd.RDD.take(RDD.scala:1435)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$first$1(RDD.scala:1476)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\r\n\tat org.apache.spark.rdd.RDD.first(RDD.scala:1476)\r\n\tat org.apache.spark.ml.util.DefaultParamsReader$.loadMetadata(ReadWrite.scala:587)\r\n\tat org.apache.spark.ml.util.DefaultParamsReader.load(ReadWrite.scala:465)\r\n\tat com.johnsnowlabs.nlp.FeaturesReader.load(ParamsAndFeaturesReadable.scala:31)\r\n\tat com.johnsnowlabs.nlp.FeaturesReader.load(ParamsAndFeaturesReadable.scala:24)\r\n\tat com.johnsnowlabs.nlp.pretrained.ResourceDownloader$.downloadModel(ResourceDownloader.scala:515)\r\n\tat com.johnsnowlabs.nlp.pretrained.ResourceDownloader$.downloadModel(ResourceDownloader.scala:507)\r\n\tat com.johnsnowlabs.nlp.pretrained.PythonResourceDownloader$.downloadModel(ResourceDownloader.scala:714)\r\n\tat com.johnsnowlabs.nlp.pretrained.PythonResourceDownloader.downloadModel(ResourceDownloader.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\n"
     ]
    }
   ],
   "source": [
    "#Definizione della pipeline per il Sentence Embedding con BERT\n",
    "model = nlpUtils.nlp_pipeline_bert_sentence_embedding(\"descrizione\").fit(df)\n",
    "result_bert = model.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d51b6a7-cd51-45b2-9448-bfa77e211610",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Caching della Sentence Embedding\n",
    "result_bert.cache()\n",
    "result_bert.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "319da7b1-0ff6-41bb-8b03-ab1d852c394c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"# dataset originale: {}\".format(df.count()))\n",
    "print(\"# dataset nuovo: {}\".format(result_bert.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b767374b-dd1a-4a59-aeb5-e26c9b3edd44",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Converte gli embedding delle sentences in colonne separate nel DataFrame \n",
    "# (ogni vettore N-dimensionale viene tramutato in un dataframe di N colonne)\n",
    "result_df_exp=nlpUtils.convert_sentence_embedding_in_col(result_bert,[\"idcase\",\"descrizione\",\"dataapertura\"])\n",
    "result_df_exp.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "827053b8-5f35-47d8-b2b8-f56dcb7b5f06",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Applichiamo la PCA sulle features ottenute dal sentence embedding (explode dell'embedding)\n",
    "col_features=result_df_exp.columns[4:]\n",
    "result_df_exp_filled = result_df_exp.dropna()\n",
    "result, pca_model, loadings=cltUtils.pipelineStandardPCA(result_df_exp_filled, col_features, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf501aa5-a8f2-438d-8724-207a984f9029",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Plottiamo la varianza mantenuta in base al numero di PCA da scegliere\n",
    "cltUtils.cumulativePCwithVariance(pca_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96b945e1-7e4b-4060-9f1b-9fd0da9697d0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Plot della Silhuette per decidere il corretto valore K di cluster\n",
    "cltUtils.silhouetteClusteringKMeans(result,\"pca_features\",m=2,n=20,i=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97ecddfa-4249-434b-8fe0-a56687b2c5bd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Applicazione del clustering\n",
    "predictions_cluster_final, final_model=cltUtils.defineClustering(result, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48f86aec-2d1d-4943-b463-ee37c7076ebe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "predictions_cluster_final.select(\"idcase\",\"descrizione\",\"dataapertura\",\"sentence\",\"prediction\").distinct().display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d0d6552-b74d-4926-a91e-746400bf3927",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Plot in 3D interattivo delle 3 PC con varianza maggiore (PC1 - PC2 - PC3) \n",
    "cltUtils.plotPCA3DInterattivo(predictions_cluster_final.where(\"dataapertura >= '2024-06-01'\"), \n",
    "                              features='pca_features', predictions='prediction', additional_column='descrizione')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fca92864-4eae-40f1-a03b-394b726c6a75",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Plot in 3D interattivo delle 3 PC con varianza maggiore (PC1 - PC2 - PC3) marcate per cluster associato\n",
    "# I punti dello spazio associati allo stesso cluster hanno lo stesso colore associato \n",
    "cltUtils.plotClustering3DInterattivo(predictions_cluster_final.where(\"dataapertura >= '2024-06-01'\"), \n",
    "                                     features='pca_features', predictions='prediction', additional_column='descrizione')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "206d9a05-68c5-4adf-9405-26518a6f43ba",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Applicazione della TF-IDF per ogni cluster:\n",
    "# Restituisce le prime N parole con il peso TF-IDF più alto per ciascuna predizione\n",
    "docs_per_topic = predictions_cluster_final.groupby('prediction').agg(concat_ws(' ', collect_list(col(\"sentence\"))).alias('Doc'))\n",
    "\n",
    "topN=nlpUtils.top_n_words(docs_per_topic, inputCol=\"Doc\", outputCol=\"features\", ngram=3, N=10, targetCol=\"prediction\")\n",
    "\n",
    "topN.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d3b304a-3d4d-43a9-9bd7-1eb15eb8d336",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "docs_per_topic = predictions_cluster_final.groupby('prediction').agg(concat_ws(' ', collect_list(col(\"sentence\"))).alias('Doc'))\n",
    "\n",
    "topN_4gram=nlpUtils.top_n_words(docs_per_topic, inputCol=\"Doc\", outputCol=\"features\", ngram=4, N=10, targetCol=\"prediction\")\n",
    "\n",
    "topN_4gram.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d8fc6e8-bfb2-4923-ad08-d1ab365d8cff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "predictions_cluster_final.groupBy(\"prediction\").count().display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67c5190c-3c78-48d0-a67a-2729c871c7c2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Creazione del dataset finale con frase in linguaggio naturale e cluster associato\n",
    "predictions_cluster_final.select(\"idcase\",\n",
    "    \"descrizione\",\"prediction\").join(topN,\"prediction\",\"left\").display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "PDB_FV_Motivi_Regolarizzazione",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
